{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Assignment_Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleaning\n",
        "Datasets may have missing values, and this can cause problems for many machine learning\n",
        "algorithms. As such, it is good practice to identify and replace missing values for each column in\n",
        "your input data prior to modeling your prediction task. This is called missing data imputation,\n",
        "or imputing for short.\n",
        "You will practice some of the data cleaning aspects while reviewing the Horse colic dataset"
      ],
      "metadata": {
        "id": "g-p6QTyu0wiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Horse Colic Dataset\n",
        "The horse colic dataset describes medical characteristics of horses with colic and whether they\n",
        "lived or died. There are 300 rows and 26 input variables with one output variable. It is a binary\n",
        "classification prediction task that involves predicting 1 if the horse lived and 2 if the horse died.\n",
        "There are many fields we could select to predict in this dataset. In this case, we will predict\n",
        "whether the problem was surgical or not (column index 23), making it a binary classification\n",
        "problem. The dataset has numerous missing values for many of the columns where each missing\n",
        "value is marked with a question mark character (\"\\?\").\n",
        "\n",
        "You can learn more about the dataset\n",
        "here:\n",
        "* Horse Colic Dataset ([horse-colic.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv))\n",
        "* Horse Colic Dataset Description ([horse-colic.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.names))\n",
        "\n",
        "The description of Horse Colic Dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)."
      ],
      "metadata": {
        "id": "jXjaSchr1ySE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting doing any coding, lets check the type of data this dataset consists of.\n",
        "Review the different attributes described in the metadata file (horse-colic.names) and indicate what type of data (categorical, numeric, etc) is present in this dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "l-5bHmqiYvtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Horse Colic data files"
      ],
      "metadata": {
        "id": "iHWVRTL22FUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the horse-colic.csv file and call it horse-colic.csv using !wget command\n",
        "#Download the horse-colic.names and call it horse-colic.names using !wget command\n",
        "\n"
      ],
      "metadata": {
        "id": "rpOkYY4I2IRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A-Loading and summarizing a dataset"
      ],
      "metadata": {
        "id": "FLdOrovd2vBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import read_csv from pandas\n",
        "\n",
        "# load dataset into variable call dataframe. Remember to check if you have headers or not.\n",
        "# Indicate the columns headings, listed here for convenience ('surgery', 'age', 'hospital_number', 'rectal temperature', 'pulse', 'respiratory rate', 'temperature of extremities', 'peripheral pulse', 'mucous membranes', 'capillary refill time', 'pain', 'peristalsis', 'abdominal distension', 'nasogastric tube', 'nasogastric reflux', 'nasogastric reflux PH', 'rectal examination - feces', 'abdomen', 'packed cell volume', 'total protein', 'abdominocentesis appearance', 'abdomcentesis total protein', 'outcome', 'surgical lesion?', 'type of lesion', 'type of lesion 2', 'type of lesion 3', 'cp_data')\n",
        "dataframe = <your input>\n",
        "dataframe.columns = <your input>\n",
        "# Q1-what is the shape of the dataframe?\n",
        "<your input>"
      ],
      "metadata": {
        "id": "ljOrWPZluTUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the first 20 rows\n",
        "<your input>\n",
        "\n",
        "# Q2-What is the meaning of '?' in this dataset?\n"
      ],
      "metadata": {
        "id": "NJTghRW7Vx6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas and numpy libraries\n",
        "# create a variable called dataframe2 where  '?' is replaced with nan from numpy. Tip use replace() and also remember nan-related functions are in numpy.\n",
        "\n",
        "dataframe2 = <your input>\n",
        "#check that in your new dataframe ? has been replaced by nan by displaying the first 20 rows.\n",
        "<your input>"
      ],
      "metadata": {
        "id": "c76SqeYDruDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now summarize the data in your new dataframe use (include='all') to view all attributes\n",
        "dataframe2.<your input>\n",
        "#Q3-Why are the counts different for each attribute?\n"
      ],
      "metadata": {
        "id": "mjLTQBsMX9_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B-Identifying missing values and duplicates"
      ],
      "metadata": {
        "id": "4aB8ZactnRq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code summarizes the number of rows with missing values for each column\n",
        "# count number of rows with missing values and give the percentage per column index. Before running the code, complete the <your input> placeholder\n",
        "for i in range(dataframe2.shape[1]):\n",
        "# complete the correct info in <your input> below\n",
        "  n_miss = dataframe2.iloc[:,i].<your input>.sum()\n",
        "  perc = n_miss / dataframe2.shape[0] * 100\n",
        "  print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))\n",
        "\n",
        "#Ignore warnings if there is any at the end of the output\n",
        "# Q4-Which attributes (name it) have more than 50 percent of missing values?\n"
      ],
      "metadata": {
        "id": "zjCCQsaDU5aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the columns with >50% percent of missing data, make sure you complete the info in the placeholders <your input>\n",
        "cols_to_drop = []\n",
        "for i in range(dataframe2.shape[1]):\n",
        "  n_miss = dataframe2.iloc[:,i].<your input>.sum()\n",
        "  perc = n_miss / dataframe2.shape[0] * 100\n",
        "  if perc > 50:\n",
        "    cols_to_drop.append(dataframe2.columns[i])\n",
        "# Now drop the columns outside the loop\n",
        "dataframe2.drop(cols_to_drop, axis=<your input>, inplace=True)\n",
        "\n",
        "#Q5-What is the shape of the dataframe2 now?\n",
        "<your input>\n"
      ],
      "metadata": {
        "id": "tIGzl9E8ul0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for duplicates in rows"
      ],
      "metadata": {
        "id": "i8L6UwtLmTTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first number of duplicated rows in dataframe2\n",
        "# Q6-How many duplicated rows are there in this dataset?\n",
        "duplicated_rows = <your input>\n",
        "print(duplicated_rows)"
      ],
      "metadata": {
        "id": "HXyI1WqtmRK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicate rows and report the shape\n",
        "dataframe2.<your input>(inplace=True)\n",
        "\n",
        "# Q7- what is the shape of dataframe2 after this?\n",
        "\n",
        "<your input>\n",
        "\n"
      ],
      "metadata": {
        "id": "cm_cjSPFmR2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C-Split data into input and output"
      ],
      "metadata": {
        "id": "DlhrnnNInG4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we want to predict whether a lesion is surgical. The output is in the 'surgical lesion?' column\n",
        "Q8- What column index in dataframe2 does 'surgical lesion?' corresponds to?"
      ],
      "metadata": {
        "id": "Pb-V2ch4J0yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into input and output (what we are predicting)\n",
        "# The column entitled surgical lesion? is the output\n",
        "# we will first create an ndarray under data variable to extract the values from our dataframe2\n",
        "data = <your input>.values\n",
        "\n",
        "# we will define X as our input dataset and y as the ouput dataset\n",
        "# to extract columns other than 'surgical lesion?' which is the output, there are various strategies.\n",
        "# One is the following:collecting the columns from 'data' ndarray other than 'surgical lesion?' in a variable called ix, then, define X as all rows and the columns in ix\n",
        "# for y we will extract the 'surgical lesion?' column from data\n",
        "\n",
        "ix = [i for i in range(data.shape[1]) if i != <output column index>]\n",
        "X= data[:,ix]\n",
        "y= data[:,<output column index>]\n",
        "X, y\n",
        "\n"
      ],
      "metadata": {
        "id": "zx1rjL8WnmVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D-Exploring statistical imputation methods"
      ],
      "metadata": {
        "id": "U_wBlwoUMX9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Statistical Imputation With SimpleImputer"
      ],
      "metadata": {
        "id": "NmW2YPFh3Srr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets try one statistical imputation method transform for our dataset\n",
        "# you will need to import SimpleImputer from sklearn.impute library\n",
        "#<your input>\n",
        "from sklearn.impute import SimpleImputer\n",
        "# define imputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "# fit on the input dataset\n",
        "imputer.fit(<input dataset>)\n",
        "# transform the dataset\n",
        "Xtrans = imputer.transform(<input dataset>)\n",
        "# summarize total missing\n",
        "print('Missing new: %d' % sum(isnan(Xtrans).flatten()))\n",
        "\n",
        "#Q9-How many missing values are there?\n",
        "\n"
      ],
      "metadata": {
        "id": "SKrEca4Q3Xyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SimpleImputer and Model Evaluation\n",
        "\n",
        "It is a good practice to evaluate machine learning models on a dataset using k-fold cross-\n",
        "validation. To correctly apply statistical missing data imputation and avoid data leakage, it is\n",
        "required that the statistics calculated for each column are calculated on the training dataset\n",
        "only, then applied to the train and test sets for each fold in the dataset."
      ],
      "metadata": {
        "id": "kKdPVwpp3fgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate mean imputation and random forest for the horse colic dataset. Here are the new libraries/functions that you need to import\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#If you check, the datatype for y, it is an object, so you need to change it to integers before moving forward with modeling\n",
        "y=y.astype(<your input>)\n",
        "\n",
        "# now we can define modeling pipeline\n",
        "model = RandomForestClassifier()\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
        "# define model evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "\n",
        "# Q10-What is the accuracy of the model?"
      ],
      "metadata": {
        "id": "NQi9nJi134p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparing Different Imputed Statistics\n",
        "\n",
        "We can design an experiment to test\n",
        "different statistical imputation strategies and discover what works best for this dataset. We will compare the mean,\n",
        "median, mode (most frequent) strategies. The mean accuracy of each approach\n",
        "can then be compared."
      ],
      "metadata": {
        "id": "H7VDudTY4Mhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of the run, a box and whisker plot is created for each set of results, allowing the\n",
        "distribution of results to be compared.\n",
        "\n"
      ],
      "metadata": {
        "id": "DUtf7mLtVycE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare statistical imputation strategies for the horse colic dataset and visualize the evaluation\n",
        "# here are the additional libraries/functions that we need to import\n",
        "# given that you already defined the X and y datasets, we can start with the pipeline\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot\n",
        "# We will try 3 different strategies for statistical imputation: the mean, the median and most frequent value\n",
        "# We will evaluate each strategy on the dataset. We will use again a Random Forest Classifier\n",
        "results = list()\n",
        "strategies = ['mean', 'median', 'most_frequent']\n",
        "for s in strategies:\n",
        "# create the modeling pipeline\n",
        "  pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m',RandomForestClassifier())])\n",
        "  # evaluate the model\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  # store results\n",
        "  results.append(scores)\n",
        "  print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=strategies, showmeans=True)\n",
        "pyplot.show()\n",
        "\n",
        "#Q11-Which imputation strategy gave the best result?"
      ],
      "metadata": {
        "id": "zvyrqa3D4RNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D-Applying best imputation strategy for making a prediction"
      ],
      "metadata": {
        "id": "p7yQfwo1MjHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SimpleImputer Transform When Making a Prediction\n",
        "\n",
        "We may wish to create a final modeling pipeline with the best scoring imputation strategy and the\n",
        "random forest algorithm, then make a prediction for new data. This can be achieved by defining\n",
        "the pipeline and fitting it on all available data, then calling the predict() function passing\n",
        "new data in as an argument. Importantly, the row of new data must mark any missing values\n",
        "using the NaN value."
      ],
      "metadata": {
        "id": "t6iOUTeb4vKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use best imputation strategy you got in previous step (best score) and prediction for the horse colic dataset\n",
        "\n",
        "# create the modeling pipeline\n",
        "pipeline = Pipeline(steps=[('i', SimpleImputer(strategy='<your input>')), ('m',\n",
        "RandomForestClassifier())])\n",
        "# fit the model\n",
        "pipeline.fit(<your input>)\n",
        "# define new data\n",
        "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, 3, 5, 45.00,\n",
        "8.40, 2, 11300, 00000, 00000, 2]\n",
        "# make a prediction\n",
        "yhat = pipeline.predict([row])\n",
        "# summarize prediction\n",
        "print('Predicted Class: %d' % yhat[0])\n",
        "\n",
        "#Q12-What outcome did the model predict for the new data?\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFZS00JR43ac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}