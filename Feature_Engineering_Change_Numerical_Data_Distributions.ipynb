{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Feature_Engineering_Change_Numerical_Data_Distributions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Change Numerical Data Distributions**\n",
        "\n"
      ],
      "metadata": {
        "id": "mfyjWBlmvoQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical input variables may have a highly skewed or non-standard distribution. This could be\n",
        "caused by outliers in the data, multi-modal distributions, highly exponential distributions, and\n",
        "more. Many machine learning algorithms prefer or perform better when numerical input variables\n",
        "and even output variables in the case of regression have a standard probability distribution,\n",
        "such as a Gaussian (normal) or a Uniform distribution.\n",
        "\n",
        "The quantile transform provides an automatic way to transform a numeric input variable to\n",
        "have a different data distribution, which in turn, can be used as input to a predictive model.\n",
        "\n",
        "In this tutorial, you will learn:\n",
        "\n",
        "* Many machine learning algorithms prefer or perform better when numerical variables have\n",
        "a Gaussian or standard probability distribution.\n",
        "* Quantile transforms are techniques for transforming numerical input or output variables\n",
        "to have a Gaussian or Uniform probability distribution.\n",
        "* How to use the QuantileTransformer to change the probability distribution of numeric\n",
        "variables to improve the performance of predictive models.\n",
        "\n",
        "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
        "\n",
        "This activity was prepared for the Practical AI/ML for Computational Biology and Chemistry Workshop (June 13-17, 2022, UD) https://github.com/udel-cbcb/al_ml_workshop\n",
        "\n",
        "Code explanation has been enriched using [CS50.ai](https://cs50.ai/)"
      ],
      "metadata": {
        "id": "PzI4Y70AXgqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Quantile Transforms\n",
        "A quantile transform is a method that transforms the data to follow a uniform or a normal distribution. This is done by mapping the original data to its quantile values. In lay terms, it's like adjusting the data so that it fits into a specific shape or pattern, either uniform (where all outcomes are equally likely) or normal (where outcomes are most likely to be in the middle, less likely at the extremes).\n",
        "\n",
        "In machine learning, this can be useful for making the data more suitable for certain algorithms. Some algorithms assume that the input data follows a normal distribution, and they may not perform well if this assumption is violated. By applying a quantile transform, we can make the data more compatible with these algorithms.\n",
        "\n",
        "The transformation\n",
        "can be applied to each numeric input variable in the training dataset and then provided as\n",
        "input to a machine learning model to learn a predictive modeling task. This quantile transform\n",
        "is available in the scikit-learn Python machine learning library via the **QuantileTransformer**\n",
        "class."
      ],
      "metadata": {
        "id": "T2VYgduehP-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show this, we first create a sample of 1,000 random Gaussian values and add a\n",
        "skew to the dataset. A histogram is created from the skewed dataset clearly showing the\n",
        "distribution pushed to the far left.\n",
        "\n"
      ],
      "metadata": {
        "id": "DTwqhjNhhvIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstration of the quantile transform\n",
        "from numpy import exp\n",
        "from numpy.random import randn\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate gaussian data sample\n",
        "data = randn(1000)\n",
        "# add a skew to the data distribution by transforming the data from a normal distribution to a log-normal distribution\n",
        "data = exp(data)\n",
        "# histogram of the raw data with a skew\n",
        "pyplot.hist(data, bins=25)\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "zYm-CnVzhiBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then a QuantileTransformer is used to map the data to a Gaussian distribution and standardize the result, centering the values on the mean value of 0 and a standard deviation of\n",
        "1.0. A histogram of the transform data is created showing a Gaussian shaped data distribution."
      ],
      "metadata": {
        "id": "pWXYD6xFiCP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this reshapes the data into a 2D array with as many rows as there are elements in data and 1 column.\n",
        "# This is necessary because many functions in sklearn expect data in this format.\n",
        "data = data.reshape((len(data),1))\n",
        "print(data.shape)\n",
        "# quantile transform the raw data\n",
        "# perform a normal quantile transform of the dataset\n",
        "# 'output_distribution\" is the orginal distribution for the transformed data. The choices are\n",
        "# 'uniform' (default) or 'normal'.\n",
        "quantile = QuantileTransformer(output_distribution='normal')\n",
        "data_trans = quantile.fit_transform(data)\n",
        "# histogram of the transformed data\n",
        "pyplot.hist(data_trans, bins=25)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "_22_ZUJyh87F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now a real example\n",
        "##Diabetes Dataset\n",
        "This dataset we used in the data preparation module. This dataset classifies patient data as\n",
        "either an onset of diabetes within five years or not. There are 768 examples and eight input variables. It is a binary classification problem.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "\n",
        "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
        "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
      ],
      "metadata": {
        "id": "cMcakc3q7Ugj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Diabetes data files"
      ],
      "metadata": {
        "id": "a70RNbHC7YnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -O pima-indians-diabetes.csv\n",
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -O pima-indians-diabetes.names\n",
        "!head pima-indians-diabetes.csv"
      ],
      "metadata": {
        "id": "FkmpdWqC7e7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summarizing the variables from the pima-indians-diabetes dataset"
      ],
      "metadata": {
        "id": "A29IdEOkjmcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and summarize the dataset\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# summarize the shape of the dataset\n",
        "print(dataset.shape)\n",
        "# summarize each variable\n",
        "print(dataset.describe())\n"
      ],
      "metadata": {
        "id": "DmuyrN1NjuB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms the 8\n",
        "input variables, one output variable, and 768 rows of data."
      ],
      "metadata": {
        "id": "K321kLM3j3uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally a histogram is created for each input variable. If we ignore the clutter of the plots and\n",
        "focus on the histograms themselves, we can see that many variables have a skewed distribution.\n",
        "The dataset provides a good candidate for using a quantile transform to make the variables\n",
        "more-Gaussian."
      ],
      "metadata": {
        "id": "wPToC_a5j-1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "g-Cu-7GNj6_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's first and evaluate a machine learning model on the raw dataset. We will use\n",
        "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
        "stratified k-fold cross-validation."
      ],
      "metadata": {
        "id": "OiLytLgQkDb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate KNN classifier on the raw dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# KFold\n",
        "#   is a cross-validator that divides the dataset into k folds.\n",
        "# Stratified\n",
        "#   is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
        "# Repeated\n",
        "#   provides a way to improve the estimated performance of a machine learning model.\n",
        "# This involves simply repeating the cross-validation procedure multiple times and reporting the mean\n",
        "# result across all folds from all runs. This mean result is expected to be a more accurate estimate\n",
        "# of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error.\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define and configure the model using\n",
        "# Classifier implementing the k-nearest neighbors vote.\n",
        "model = KNeighborsClassifier()\n",
        "# evaluate the model using RepeatedStratifiedKFold cross validator,\n",
        "# that repeats Stratified K-Fold n times with different randomization in each\n",
        "# repetition.\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report model performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "RAHVPhnNkGyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we can see that the model achieved a mean classification accuracy of about 71.7\n",
        "percent."
      ],
      "metadata": {
        "id": "7CK6RYW6kX7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normal Quantile Transform\n",
        "It is often desirable to transform an input variable to have a normal probability distribution to improve the modeling performance. We can apply the Quantile transform using the\n",
        "QuantileTransformer class and set the output distribution argument to `normal'. We\n",
        "must also set the n quantiles argument to a value less than the number of observations in the\n",
        "training dataset, in this case, 100. Once defined, we can call the fit transform() function and\n",
        "pass it to our dataset to create a quantile transformed version of our dataset."
      ],
      "metadata": {
        "id": "WERxO5mNkejX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize a normal quantile transform of the dataset\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve just the numeric input values\n",
        "data = dataset.values[:, :-1]\n",
        "# perform a normal quantile transform of the dataset\n",
        "# 'n_quantiles\" is the number of quantiles to be computed. It corresponds to the number\n",
        "# of landmarks used to discretize the cumulative distribution function.\n",
        "# 'output_distribution\" is the arginal distribution for the transformed data. The choices are\n",
        "# 'uniform' (default) or 'normal'.\n",
        "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
        "data = trans.fit_transform(data)\n",
        "# convert the array back to a dataframe\n",
        "dataset = DataFrame(data)\n",
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "E2k2fd5Rki3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the shape of the histograms for each variable looks very Gaussian as compared\n",
        "to the raw data."
      ],
      "metadata": {
        "id": "YvSP_sU1ktz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
        "normal quantile transform of the dataset"
      ],
      "metadata": {
        "id": "g2agVpCtkzcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate KNN with normal quantile transform\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define the pipeline\n",
        "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
        "# Classifier implementing the k-nearest neighbors vote.\n",
        "model = KNeighborsClassifier()\n",
        "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
        "# evaluate the pipeline\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report pipeline performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "PUEXSTdck2y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that the normal quantile transform results in a lift in\n",
        "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
        "transform."
      ],
      "metadata": {
        "id": "LalexVZVk-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Uniform Quantile Transform\n",
        "Sometimes it can be beneficial to transform a highly exponential or multi-modal distribution to\n",
        "have a uniform distribution. This is especially useful for data with a large and sparse range of\n",
        "values, e.g. outliers that are common rather than rare. We can apply the transform by defining\n",
        "a QuantileTransformer class and setting the output distribution argument to `uniform'\n",
        "(the default)."
      ],
      "metadata": {
        "id": "vq14snT7lF1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize a uniform quantile transform of the dataset\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve just the numeric input values\n",
        "data = dataset.values[:, :-1]\n",
        "# perform a uniform quantile transform of the dataset\n",
        "trans = QuantileTransformer(n_quantiles=100, output_distribution='uniform')\n",
        "data = trans.fit_transform(data)\n",
        "# convert the array back to a dataframe\n",
        "dataset = DataFrame(data)\n",
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "FL5MIL4qlJ5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the shape of the histograms for each variable looks very uniform compared to\n",
        "the raw data."
      ],
      "metadata": {
        "id": "e4Tl4N72lPW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
        "uniform quantile transform of the raw dataset."
      ],
      "metadata": {
        "id": "1wcC9rl-lTGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate KNN classifer on the dataset with uniform quantile transform\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define the pipeline\n",
        "trans = QuantileTransformer(n_quantiles=100, output_distribution='uniform')\n",
        "model = KNeighborsClassifier()\n",
        "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
        "# evaluate the pipeline\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report pipeline performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "uAatU9wTlYm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that the uniform transform results in a lift in performance\n",
        "from 71.7 percent accuracy without the transform to about 73.4 percent with the normal transform, and achieved a score of 74.1 percent."
      ],
      "metadata": {
        "id": "BUzVORvvlg2U"
      }
    }
  ]
}