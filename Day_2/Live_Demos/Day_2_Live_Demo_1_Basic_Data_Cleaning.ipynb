{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_2_Live_Demo_1_Basic_Data_Cleaning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Basic Data Cleaning**"
      ],
      "metadata": {
        "id": "sFcYiVhbweM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, you will learn:\n",
        "\n",
        "* How to identify and remove column variables that only have a single value.\n",
        "* How to identify and consider column variables with very few unique values.\n",
        "* How to identify and remove rows that contain duplicate observations.\n",
        "\n",
        "Adpated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
      ],
      "metadata": {
        "id": "T-xjYFrCQhmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Messy Dataset\n",
        "\n",
        "\n",
        "Breast cancer dataset classifies breast cancer\n",
        "patient as either a recurrence or no recurrence of cancer. There are 286 examples and nine\n",
        "input variables.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
        "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "giZ-tn6J7ktc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise: Review the content and description of the Breast cancer dataset by cliking on the links\n",
        "#The !wget command is used to download files from the internet. Format: !wget \"URL\" -O filename.csv. The -O option in the wget command is used to specify the name of the file that you want to save the downloaded content as. In this case, filename.csv is the name of the file where the content from the URL will be saved. Please note that it's not -0 (zero), it's -O (capital o).\n",
        "#download the breast-cancer.csv file and save it as bcancer_data.csv. Then print the first rows of the file.\n",
        "\n",
        "#Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "PNbQBqxZaNaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now get read the csv using pandas (remember last module)\n",
        "\n",
        "#Your code here"
      ],
      "metadata": {
        "id": "eIx64PXxfDO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's add the column labels (check on the breast-cancer.names link)\n",
        "\n",
        "#your code here"
      ],
      "metadata": {
        "id": "CA2Mlb8mjc9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the stats for imported data\n",
        "\n",
        "#your code here"
      ],
      "metadata": {
        "id": "AofvcO7-kkqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download messy data file"
      ],
      "metadata": {
        "id": "ibIiSW0g-r4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to work with a modified version of the  from Breast Cancer Dataset. It contains an additional column with a unique value. If there is a column that has a constant value across all observations, this is not going to affect the prediction. Such features thus can be removed. This is to demonstrate how to remove such information."
      ],
      "metadata": {
        "id": "eTuuFkqMZ1qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\" -O messy_data.csv\n",
        "!head messy_data.csv"
      ],
      "metadata": {
        "id": "4MvsLbWB-hSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identify Columns That Contain a Single Value\n"
      ],
      "metadata": {
        "id": "sRe1fc_I-MQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the number of unique values for each column using pandas, using .nunique\n",
        "# load the dataset\n",
        "df = pd.read_csv('messy_data.csv', header=None)\n",
        "# summarize the number of unique values in each column using nunique()\n",
        "print(\"Shape of messy data: \", df.shape)\n",
        "print(\"Column\\t#Unique values \")\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "id": "Fz3fYIjp_ea4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that column index 5 only has a single value and should be removed as it won't influence the prediction."
      ],
      "metadata": {
        "id": "OpjQlWtH_qcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Delete columns that contain a single value"
      ],
      "metadata": {
        "id": "1bPtTttf_vga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete column [5] that contains a single unique value\n",
        "# your code here"
      ],
      "metadata": {
        "id": "KZm38r5wIIsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, when you don't know which ones should be dropped\n",
        "# load the dataset\n",
        "df = pd.read_csv('messy_data.csv', header=None)\n",
        "print(df.shape)\n",
        "# get number of unique values for each column\n",
        "counts = df.nunique()\n",
        "# record columns to delete: This is a list comprehension in Python. It creates a new list, to_del, from the indices i of the counts list where the value v is equal to 1. In other words, it's finding the positions of all elements in counts that are equal to 1.\n",
        "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
        "print(to_del)\n",
        "# drop useless columns\n",
        "df.drop(to_del, axis=1, inplace=True)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "738Abx8f_0-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identify columns that have very few values"
      ],
      "metadata": {
        "id": "i5bW7pFsAI3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('messy_data.csv', header=None)\n",
        "# summarize the number of unique values in each column\n",
        "print(\"Column, Count, <1%\")\n",
        "for i, v in enumerate(df.nunique()):\n",
        "  percentage = float(v) / df.shape[0] * 100\n",
        "  if percentage < 1:\n",
        "    print('%d, %d, %.1f%%' % (i, v, percentage))"
      ],
      "metadata": {
        "id": "qW3BwXIOFbfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Drop columns with unique values less than 1 percent of rows"
      ],
      "metadata": {
        "id": "vnjw39eIF2kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete columns where number of unique values is less than 1% of the rows\n",
        "from pandas import read_csv\n",
        "# load the dataset\n",
        "df = read_csv('messy_data.csv', header=None)\n",
        "print(df.shape)\n",
        "# get number of unique values for each column\n",
        "counts = df.nunique()\n",
        "# record columns to delete\n",
        "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0] * 100) < 1]\n",
        "print(\"Columns to delete: \", to_del)\n",
        "# drop useless columns\n",
        "df.drop(to_del, axis=1, inplace=True)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "KiPY_mBqGGeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identify rows that contain duplicate data"
      ],
      "metadata": {
        "id": "fOPZfL-JPe7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset and add column headers\n",
        "df = pd.read_csv('messy_data.csv', header=None)\n",
        "df.columns = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-cap', 'additional', 'deg-malig', 'breast', 'breast-quad', 'irradiat', 'class']\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "WFrSFoZRPnpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate duplicates\n",
        "# Identify and display all duplicate.\n",
        "#The duplicated() function returns a Boolean Series denoting duplicate rows.\n",
        "#The keep=False argument marks all duplicates as True. So, df[df.duplicated(keep=False)] returns all the duplicate rows in the DataFrame.\n",
        "#To print duplicates related to particular column information, then use df.duplicated(subset['col1', 'col2'], keep=False)\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "print(duplicate_rows)"
      ],
      "metadata": {
        "id": "xRy3U2OP66K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Delete rows that contain duplicate data"
      ],
      "metadata": {
        "id": "noMvFc6AP-53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete rows of duplicate data from the dataset\n",
        "# load the dataset\n",
        "df = pd.read_csv('messy_data.csv', header=None)\n",
        "print(df.shape)\n",
        "# delete duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "G4VBQ_dIQEes"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}