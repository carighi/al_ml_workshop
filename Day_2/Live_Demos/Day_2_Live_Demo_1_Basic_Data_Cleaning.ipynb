{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day_2_Live_Demo_1_Basic_Data_Cleaning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNMXthxUuyjWdhAYQgyb3Q/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Basic Data Cleaning**"],"metadata":{"id":"sFcYiVhbweM9"}},{"cell_type":"markdown","source":["In this tutorial, you will learn:\n","\n","* How to identify and remove column variables that only have a single value.\n","* How to identify and consider column variables with very few unique values.\n","* How to identify and remove rows that contain duplicate observations.\n","\n","Adpated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."],"metadata":{"id":"T-xjYFrCQhmD"}},{"cell_type":"markdown","source":["#Messy Dataset\n","\n","\n","Breast cancer dataset classifies breast cancer\n","patient data as either a recurrence or no recurrence of cancer. There are 286 examples and nine\n","input variables. \n","\n","You can learn more about the dataset here:\n","* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n","* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n","\n","\n","The messy dataset was modified from Breast Cancer Dataset.\n"],"metadata":{"id":"giZ-tn6J7ktc"}},{"cell_type":"markdown","source":["###Download messy data file"],"metadata":{"id":"ibIiSW0g-r4n"}},{"cell_type":"code","source":["!wget \"https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\" -O messy_data.csv\n","!head messy_data.csv"],"metadata":{"id":"4MvsLbWB-hSu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652627768900,"user_tz":240,"elapsed":320,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"fe4b2a0e-e1c0-4405-f2f3-2568d8f76434"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-15 15:16:08--  https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 25496 (25K) [text/plain]\n","Saving to: ‘messy_data.csv’\n","\n","\rmessy_data.csv        0%[                    ]       0  --.-KB/s               \rmessy_data.csv      100%[===================>]  24.90K  --.-KB/s    in 0.002s  \n","\n","2022-05-15 15:16:08 (15.4 MB/s) - ‘messy_data.csv’ saved [25496/25496]\n","\n","'40-49','premeno','15-19','0-2','yes',4','3','right','left_up','no','recurrence-events'\n","'50-59','ge40','15-19','0-2','no',4','1','right','central','no','no-recurrence-events'\n","'50-59','ge40','35-39','0-2','no',4','2','left','left_low','no','recurrence-events'\n","'40-49','premeno','35-39','0-2','yes',4','3','right','left_low','yes','no-recurrence-events'\n","'40-49','premeno','30-34','3-5','yes',4','2','left','right_up','no','recurrence-events'\n","'50-59','premeno','25-29','3-5','no',4','2','right','left_up','yes','no-recurrence-events'\n","'50-59','ge40','40-44','0-2','no',4','3','left','left_up','no','no-recurrence-events'\n","'40-49','premeno','10-14','0-2','no',4','2','left','left_up','no','no-recurrence-events'\n","'40-49','premeno','0-4','0-2','no',4','2','right','right_low','no','no-recurrence-events'\n","'40-49','ge40','40-44','15-17','yes',4','2','right','left_up','yes','no-recurrence-events'\n"]}]},{"cell_type":"markdown","source":["#Identify Columns That Contain a Single Value\n"],"metadata":{"id":"sRe1fc_I-MQw"}},{"cell_type":"code","source":["# summarize the number of unique values for each column using pandas\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# summarize the number of unique values in each column using nunique()\n","print(df.shape)\n","print(df.nunique())"],"metadata":{"id":"Fz3fYIjp_ea4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652627769280,"user_tz":240,"elapsed":381,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"86239904-d0d6-4a6b-fb2a-843fe061e1b3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","0      6\n","1      3\n","2     11\n","3      7\n","4      2\n","5      1\n","6      3\n","7      2\n","8      5\n","9      2\n","10     2\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["We can see that column index 5 only has a single value and should be removed."],"metadata":{"id":"OpjQlWtH_qcK"}},{"cell_type":"markdown","source":["#Delete columns that contain a single value"],"metadata":{"id":"1bPtTttf_vga"}},{"cell_type":"code","source":["# delete columns with a single unique value\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# get number of unique values for each column\n","counts = df.nunique()\n","# record columns to delete\n","to_del = [i for i,v in enumerate(counts) if v == 1]\n","print(to_del)\n","# drop useless columns\n","df.drop(to_del, axis=1, inplace=True)\n","print(df.shape)"],"metadata":{"id":"738Abx8f_0-I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652627769280,"user_tz":240,"elapsed":10,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"d13d027c-5d96-4ef7-d681-8dfdc8258a10"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","[5]\n","(289, 10)\n"]}]},{"cell_type":"markdown","source":["#Identify columns that have very few values"],"metadata":{"id":"i5bW7pFsAI3N"}},{"cell_type":"code","source":["# summarize the number of unique values for each column using pandas\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# summarize the number of unique values in each column\n","for i, v in enumerate(df.nunique()):\n","  percentage = float(v) / df.shape[0] * 100\n","  if percentage < 1:\n","    print('%d, %d, %.1f%%' % (i, v, percentage))"],"metadata":{"id":"qW3BwXIOFbfH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652627769281,"user_tz":240,"elapsed":9,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"2dc07c1e-9d7d-4d9c-fce8-b2241b777c4c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["4, 2, 0.7%\n","5, 1, 0.3%\n","7, 2, 0.7%\n","9, 2, 0.7%\n","10, 2, 0.7%\n"]}]},{"cell_type":"markdown","source":["#Drop columns with unique values less than 1 percent of rows"],"metadata":{"id":"vnjw39eIF2kD"}},{"cell_type":"code","source":["# delete columns where number of unique values is less than 1% of the rows\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# get number of unique values for each column\n","counts = df.nunique()\n","# record columns to delete\n","to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0] * 100) < 1]\n","print(to_del)\n","# drop useless columns\n","df.drop(to_del, axis=1, inplace=True)\n","print(df.shape)"],"metadata":{"id":"KiPY_mBqGGeQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652627769281,"user_tz":240,"elapsed":6,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"212da44f-3365-407c-8cae-a0463f0a7c6e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","[4, 5, 7, 9, 10]\n","(289, 6)\n"]}]},{"cell_type":"markdown","source":["#Identify rows that contain duplicate data"],"metadata":{"id":"fOPZfL-JPe7U"}},{"cell_type":"code","source":["# locate rows of duplicate data\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# calculate duplicates\n","dups = df.duplicated()\n","# report if there are any duplicates\n","print(dups.any())\n","# list all duplicate rows\n","print(df[dups])"],"metadata":{"id":"WFrSFoZRPnpj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652627769450,"user_tz":240,"elapsed":174,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"2cc8b3b5-7977-474c-a171-cab8056d4467"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","          0          1        2      3      4   5    6        7           8   \\\n","17   '60-69'     'ge40'  '15-19'  '0-2'   'no'  4'  '2'  'right'   'left_up'   \n","27   '40-49'  'premeno'  '10-14'  '0-2'   'no'  4'  '1'  'right'   'left_up'   \n","44   '30-39'  'premeno'  '15-19'  '0-2'   'no'  4'  '1'   'left'  'left_low'   \n","65   '50-59'     'ge40'  '15-19'  '0-2'   'no'  4'  '1'  'right'   'central'   \n","117  '60-69'     'ge40'  '10-14'  '0-2'   'no'  4'  '1'   'left'   'left_up'   \n","178  '40-49'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'  'right'  'left_low'   \n","190  '50-59'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'   'left'  'right_up'   \n","214  '40-49'  'premeno'  '20-24'  '0-2'   'no'  4'  '2'  'right'   'left_up'   \n","217  '50-59'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","221  '50-59'     'ge40'  '20-24'  '0-2'   'no'  4'  '3'   'left'   'left_up'   \n","237  '30-39'     'lt40'  '15-19'  '0-2'   'no'  4'  '3'  'right'   'left_up'   \n","240  '50-59'     'ge40'  '40-44'  '6-8'  'yes'  4'  '3'   'left'  'left_low'   \n","246  '60-69'     'ge40'  '15-19'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","268  '30-39'  'premeno'  '35-39'  '0-2'   'no'  4'  '3'   'left'  'left_low'   \n","270  '60-69'     'ge40'  '20-24'  '0-2'   'no'  4'  '1'   'left'  'left_low'   \n","287  '40-49'  'premeno'  '20-24'  '0-2'   'no'  4'  '2'  'right'  'right_up'   \n","288  '50-59'     'ge40'  '40-44'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","\n","        9                       10  \n","17    'no'  'no-recurrence-events'  \n","27    'no'  'no-recurrence-events'  \n","44    'no'  'no-recurrence-events'  \n","65    'no'  'no-recurrence-events'  \n","117   'no'  'no-recurrence-events'  \n","178   'no'     'recurrence-events'  \n","190   'no'     'recurrence-events'  \n","214   'no'  'no-recurrence-events'  \n","217   'no'  'no-recurrence-events'  \n","221   'no'  'no-recurrence-events'  \n","237   'no'  'no-recurrence-events'  \n","240  'yes'     'recurrence-events'  \n","246   'no'  'no-recurrence-events'  \n","268   'no'     'recurrence-events'  \n","270   'no'  'no-recurrence-events'  \n","287   'no'  'no-recurrence-events'  \n","288   'no'  'no-recurrence-events'  \n"]}]},{"cell_type":"markdown","source":["#Delete rows that contain duplicate data"],"metadata":{"id":"noMvFc6AP-53"}},{"cell_type":"code","source":["# delete rows of duplicate data from the dataset\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# delete duplicate rows\n","df.drop_duplicates(inplace=True)\n","print(df.shape)"],"metadata":{"id":"G4VBQ_dIQEes","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652627769450,"user_tz":240,"elapsed":4,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"7dd27e7b-6213-4516-f937-f1af51c2a9f4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","(272, 11)\n"]}]}]}