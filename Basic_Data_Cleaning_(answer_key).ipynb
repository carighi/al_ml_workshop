{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Basic_Data_Cleaning_(answer_key).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Basic Data Cleaning**"
      ],
      "metadata": {
        "id": "sFcYiVhbweM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning is basically the “process of fixing or removing data that is inaccurate, duplicated, or outside the scope of your research question” (https://datascience.cancer.gov/training/learn-data-science/clean-data-basics).\n",
        "It is a very time consuming task. Thus, data scientists usually spend significant time getting the datasets in a final form that they can work in subsequent steps.\n",
        "\n",
        "It is important to be able to deal with messy data, whic includes missing values, inconsistent formatting, malformed records, or nonsensical outliers.\n",
        "We will leverage pandas and NumPy libraries to perform some data cleaning steps.\n",
        "\n",
        "\n",
        "In this tutorial, you will learn:\n",
        "\n",
        "* How to identify and remove column variables that only have a single value.\n",
        "* How to remove columns with duplicated information.\n",
        "* How to identify and remove rows that contain duplicate observations.\n",
        "\n",
        "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
      ],
      "metadata": {
        "id": "T-xjYFrCQhmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Messy Dataset\n",
        "\n",
        "\n",
        "Breast cancer dataset classifies breast cancer\n",
        "patient as either a recurrence or no recurrence of cancer. There are 286 examples and nine\n",
        "input variables.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
        "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "giZ-tn6J7ktc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise: Review the content and description of the Breast cancer dataset by cliking on the links\n",
        "#The !wget command is used to download files from the internet. Format: !wget \"URL\" -O filename.csv. The -O option in the wget command is used to specify the name of the file that you want to save the downloaded content as. In this case, filename.csv is the name of the file where the content from the URL will be saved. Please note that it's not -0 (zero), it's -O (capital o).\n",
        "#download the breast-cancer.csv file and save it as bcancer_data.csv. Then print the first rows of the file.\n",
        "\n",
        "#Your code here\n",
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\" -O bcancer_data.csv\n",
        "!head bcancer_data.csv\n"
      ],
      "metadata": {
        "id": "PNbQBqxZaNaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now get read the csv using pandas (remember last module). Note that the file does not contain headers, so when you read the file use header=None\n",
        "\n",
        "#Your code here\n",
        "#from pandas import read_csv\n",
        "import pandas as pd\n",
        "bcancer = pd.read_csv(\"bcancer_data.csv\", header=None)\n",
        "bcancer.head()\n"
      ],
      "metadata": {
        "id": "eIx64PXxfDO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's add the column labels (check on the breast-cancer.names link). Hint: .columns function\n",
        "\n",
        "#your code here\n",
        "bcancer.columns = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-cap', 'deg-malig', 'breast', 'breast-quad', 'irradiat', 'class']\n",
        "bcancer.head()\n"
      ],
      "metadata": {
        "id": "CA2Mlb8mjc9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the stats for imported data. Hint: .describe function\n",
        "\n",
        "#your code here\n",
        "\n",
        "bcancer.describe()"
      ],
      "metadata": {
        "id": "AofvcO7-kkqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Answer the following questions:    \n",
        "\n",
        "1.   How many total observations are there in the Brease-cancer.csv database? 286\n",
        "2.   What does the unique number represent in each column? E.g., for age column unique is 6-> represents the number of unique data values/labels in the given column, in the case of age, they are 6 because they represent the 6 age ranges\n",
        "\n",
        "1.   How many observations are there for no-recurrence-events? 201\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-mVpFmV4n6Mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download messy data file"
      ],
      "metadata": {
        "id": "ibIiSW0g-r4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to work with a modified version of the Breast Cancer Dataset. It contains a couple of additional rows and columns that help to illustrate the cases below."
      ],
      "metadata": {
        "id": "eTuuFkqMZ1qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/carighi/al_ml_workshop/main/data/messy2.csv\" -O messy2.csv\n",
        "!head messy2.csv"
      ],
      "metadata": {
        "id": "4MvsLbWB-hSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identify Columns That Contain a Single Value\n"
      ],
      "metadata": {
        "id": "sRe1fc_I-MQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the number of unique values for each column using pandas, using .nunique\n",
        "# load the dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('messy2.csv', header=None)\n",
        "# summarize the number of unique values in each column using nunique()\n",
        "print(\"Shape of messy data: \", df.shape)\n",
        "print(\"Column\\t#Unique values \")\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "id": "Fz3fYIjp_ea4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that column index 5 only has a single value and should be removed as it won't influence the prediction."
      ],
      "metadata": {
        "id": "OpjQlWtH_qcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternatively\n",
        "for i in df.columns:\n",
        "  unique_val= df[i].nunique()\n",
        "  print(f'Count of column in {i}:', unique_val)"
      ],
      "metadata": {
        "id": "3-QIWNqyOpWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Delete columns that contain a single value"
      ],
      "metadata": {
        "id": "1bPtTttf_vga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete column [5] that contains a single unique value. Hint: .drop function\n",
        "# your code here\n",
        "import pandas as pd\n",
        "df2= df.drop(columns=[5])\n",
        "df2.head()\n"
      ],
      "metadata": {
        "id": "KZm38r5wIIsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, when you don't know which ones should be dropped\n",
        "# load the dataset\n",
        "#df = pd.read_csv('messy2.csv', header=None)\n",
        "#print(df.shape)\n",
        "# get number of unique values for each column using nunique\n",
        "#counts = df.nunique()\n",
        "# record columns to delete: This is a list comprehension in Python. It creates a new list, to_del, from the indices i of the counts list where the value v is equal to 1. In other words, it's finding the positions of all elements in counts that are equal to 1.\n",
        "#to_del = [i for i,v in enumerate(counts) if v == 1]\n",
        "#print(to_del)\n",
        "# drop useless columns\n",
        "#df.drop(to_del, axis=1, inplace=True)\n",
        "#print(df.shape)\n",
        "#df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "738Abx8f_0-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Identify columns that contain duplicated information.\n",
        "\n",
        "If same information appears in more than one column (note that the label of the column could be different), then we can remove one of them."
      ],
      "metadata": {
        "id": "i5bW7pFsAI3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add the labels to the columns"
      ],
      "metadata": {
        "id": "gb7elg3rgqIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add column labels to df2 and view first rows . Hint: use .columns. Here are the labels:'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-cap', 'deg-malig', 'breast', 'breast-quad', 'irradiat', 'class', 'breast_and_quadrant'\n",
        "df2.columns = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-cap', 'deg-malig', 'breast', 'breast-quad', 'irradiat', 'class', 'breast_and_quadrant']\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "qW3BwXIOFbfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the messy2 data, the last column is a combination of the breast and breast quadrant. This information is in other columns, so we can remove it."
      ],
      "metadata": {
        "id": "vnjw39eIF2kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the breast_and_quadrant column in df2 and display few first rows\n",
        "df2.drop(columns='breast_and_quadrant', inplace=True)\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "KiPY_mBqGGeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identify rows that contain duplicate data"
      ],
      "metadata": {
        "id": "fOPZfL-JPe7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate duplicates\n",
        "# Identify and display all duplicate.\n",
        "#The duplicated() function returns a Boolean Series denoting duplicate rows.\n",
        "#The keep=False argument marks all duplicates as True. So, df[df.duplicated(keep=False)] returns all the duplicate rows in the DataFrame.\n",
        "#To print duplicates related to particular column information, then use df.duplicated(subset['col1', 'col2'], keep=False)\n",
        "duplicate_rows = df2[df2.duplicated(keep=False)]\n",
        "print(duplicate_rows)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WFrSFoZRPnpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Delete rows that contain duplicate data"
      ],
      "metadata": {
        "id": "noMvFc6AP-53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete rows of duplicate data from the dataset\n",
        "# delete duplicate rows\n",
        "df2.drop_duplicates(inplace=True)\n",
        "print(df2.shape)"
      ],
      "metadata": {
        "id": "G4VBQ_dIQEes"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}