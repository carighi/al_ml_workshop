{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Feature_Scaling_Numerical_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scale Numerical Data**"
      ],
      "metadata": {
        "id": "Q_HPhXyhvx4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many machine learning algorithms perform better when numerical input variables are scaled\n",
        "to a standard range. This includes algorithms that use a weighted sum of the input, like\n",
        "linear regression, and algorithms that use distance measures, like k-nearest neighbors. The two\n",
        "most popular techniques for scaling numerical data prior to modeling are normalization and\n",
        "standardization.\n",
        "* **Normalization** scales each input variable separately to the range of 0-1, which is\n",
        "the range for\n",
        "floating-point values where we have the most precision.\n",
        "* **Standardization** scales\n",
        "each input variable separately by subtracting the mean (called centering) and dividing by the\n",
        "standard deviation to shift the distribution to have a mean of zero and a standard deviation of\n",
        "one.\n",
        "\n",
        "In this tutorial, you will learn:\n",
        "\n",
        "* Data scaling is a recommended pre-processing step when working with many machine\n",
        "learning algorithms.\n",
        "* Data scaling can be achieved by normalizing or standardizing real-valued input and output\n",
        "variables.\n",
        "* How to apply standardization and normalization to improve the performance of predictive\n",
        "modeling algorithms.\n",
        "\n",
        "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
      ],
      "metadata": {
        "id": "PzI4Y70AXgqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Numerical Data Scaling Methods\n",
        "\n"
      ],
      "metadata": {
        "id": "SKE4Zs0-a6NI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Normalization\n",
        "Normalization is a rescaling of the data from the original range so that all values are within the\n",
        "new range of 0 and 1. Normalization requires that you know or are able to accurately estimate\n",
        "the minimum and maximum observable values. You may be able to estimate these values from\n",
        "your available data."
      ],
      "metadata": {
        "id": "CmDQwyhWehGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a normalization\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# define data\n",
        "data = asarray([[100, 0.001],\n",
        "[8, 0.05],\n",
        "[50, 0.005],\n",
        "[88, 0.07],\n",
        "[4, 0.1]])\n",
        "print(data)\n",
        "# define min max scaler\n",
        "# Transform features by scaling each feature to a given range.\n",
        "# This estimator scales and translates each feature individually such\n",
        "# that it is in the given range on the training set, e.g. between\n",
        "# zero and one.\n",
        "scaler = MinMaxScaler()\n",
        "# Fit to data, then transform it.\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)"
      ],
      "metadata": {
        "id": "oKdCavZ9bKZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Standardization\n",
        "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed\n",
        "values is 0 and the standard deviation is 1. This can be thought of as subtracting the mean\n",
        "value or centering the data. Like normalization, standardization can be useful, and even\n",
        "required in some machine learning algorithms when your data has input values with differing\n",
        "scales. Standardization assumes that your observations fit a Gaussian distribution (bell curve)\n",
        "with a well-behaved mean and standard deviation. You can still standardize your data if this\n",
        "expectation is not met, but you may not get reliable results."
      ],
      "metadata": {
        "id": "FUWf9dTVbRpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a standardization\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# define data\n",
        "data = asarray([[100, 0.001],\n",
        "[8, 0.05],\n",
        "[50, 0.005],\n",
        "[88, 0.07],\n",
        "[4, 0.1]])\n",
        "print(data)\n",
        "# define standard scaler\n",
        "# Standardize features by removing the mean and scaling to unit variance.\n",
        "scaler = StandardScaler()\n",
        "# Fit to data, then transform it.\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)"
      ],
      "metadata": {
        "id": "f0UHd_kybYnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Diabetes Dataset\n",
        "The dataset classifies patient data as\n",
        "either an onset of diabetes within five years or not. There are 768 examples and eight input variables. It is a binary classification problem.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "\n",
        "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
        "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
      ],
      "metadata": {
        "id": "07P2vTkSbzbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Diabetes data files"
      ],
      "metadata": {
        "id": "Of_xObt4b878"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -O pima-indians-diabetes.csv\n",
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -O pima-indians-diabetes.names\n",
        "!head pima-indians-diabetes.csv"
      ],
      "metadata": {
        "id": "Ux33MjJncFGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and summarize the diabetes dataset\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# summarize the shape of the dataset\n",
        "print(dataset.shape)\n",
        "# summarize each variable\n",
        "print(dataset.describe())"
      ],
      "metadata": {
        "id": "PrDVjE97cUuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms the 8\n",
        "input variables, one output variable, and 768 rows of data. A statistical summary of the input\n",
        "variables is provided show that each variable has a very different scale. This makes it a good\n",
        "dataset for exploring data scaling methods.\n",
        "\n",
        "We can create a histogram for each input variable. The plots confirm the differing scale\n",
        "for each input variable and show that the variables have different scales.\n"
      ],
      "metadata": {
        "id": "dv244yUhcWZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "5b5_NZcKcpBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
        "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
        "stratified k-fold cross-validation."
      ],
      "metadata": {
        "id": "m-G2w7G5c8AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate knn on the raw diabetes dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define and configure the model\n",
        "model = KNeighborsClassifier()\n",
        "# evaluate the model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report model performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "8QGvOr06dAEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we can see that the model achieved a mean classification accuracy of about\n",
        "71.7 percent."
      ],
      "metadata": {
        "id": "0GqKjRKOdIT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MinMaxScaler Transform\n",
        "We can apply the **MinMaxScaler** to the diabetes dataset directly to normalize the input variables.\n",
        "We will use the default configuration and scale values to the range 0 and 1. First, a **MinMaxScaler**\n",
        "instance is defined with default hyperparameters. Once defined, we can call the **fit.transform**()\n",
        "function and pass it to our dataset to create a transformed version of our dataset."
      ],
      "metadata": {
        "id": "7v-tQdA2dWnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary of each input variable"
      ],
      "metadata": {
        "id": "tBRrkUMBf2J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize a minmax scaler transform of the diabetes dataset\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib import pyplot\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve just the numeric input values\n",
        "data = dataset.values[:, :-1]\n",
        "# perform a min-max scaler transform of the dataset\n",
        "trans = MinMaxScaler()\n",
        "data = trans.fit_transform(data)\n",
        "# convert the array back to a dataframe\n",
        "dataset = DataFrame(data)\n",
        "# summarize\n",
        "print(dataset.describe())"
      ],
      "metadata": {
        "id": "wcDL7d1LdiAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the\n",
        "distributions have been adjusted and that the minimum and maximum values for each variable\n",
        "are now 0.0 and 1.0 respectively."
      ],
      "metadata": {
        "id": "FAMM5middm1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Histogram plots of the variables"
      ],
      "metadata": {
        "id": "tljvL9U1f6og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "S1lShljJdtH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram plots of the variables are created, although the distributions don't look much\n",
        "different from their original distributions seen in the previous section. We can confirm that the\n",
        "minimum and maximum values are zero and one respectively, as we expected."
      ],
      "metadata": {
        "id": "S6zNYXTTdrnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model evaluation"
      ],
      "metadata": {
        "id": "fp-2ZyDRf_UB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's evaluate the same KNN model as the previous section, but in this case, on a\n",
        "MinMaxScaler transform of the dataset."
      ],
      "metadata": {
        "id": "YF-oZ7pUd9ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate knn on the diabetes dataset with minmax scaler transform\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define the pipeline\n",
        "trans = MinMaxScaler()\n",
        "model = KNeighborsClassifier()\n",
        "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
        "# evaluate the pipeline\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report pipeline performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "G2k1KcTYd-EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the MinMaxScaler transform results in a lift in\n",
        "performance from 71.7 percent accuracy without the transform to about 73.9 percent with the\n",
        "transform."
      ],
      "metadata": {
        "id": "ud8_nakzeNjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StandardScaler Transform\n",
        "We can apply the StandardScaler to the diabetes dataset directly to standardize the input\n",
        "variables. We will use the default configuration and scale values to subtract the mean to center\n",
        "them on 0.0 and divide by the standard deviation to give the standard deviation of 1.0. First, a\n",
        "StandardScaler instance is defined with default hyperparameters. Once defined, we can call\n",
        "the fit transform() function and pass it to our dataset to create a transformed version of our\n",
        "dataset."
      ],
      "metadata": {
        "id": "AMTAx2MDeoUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary of each input variable"
      ],
      "metadata": {
        "id": "xOOwy2cpfkkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize a standard scaler transform of the diabetes dataset\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib import pyplot\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve just the numeric input values\n",
        "data = dataset.values[:, :-1]\n",
        "# perform a robust scaler transform of the dataset\n",
        "trans = StandardScaler()\n",
        "data = trans.fit_transform(data)\n",
        "# convert the array back to a dataframe\n",
        "dataset = DataFrame(data)\n",
        "# summarize\n",
        "print(dataset.describe())"
      ],
      "metadata": {
        "id": "FBvsYK7fezui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the\n",
        "distributions have been adjusted and that the mean is a very small number close to zero and\n",
        "the standard deviation is very close to 1.0 for each variable."
      ],
      "metadata": {
        "id": "LMBZNTnte9_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram plots of the variables"
      ],
      "metadata": {
        "id": "_8Mpp_N4fXeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "iEvCYerze-yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram plots of the variables are created, although the distributions don't look much\n",
        "different from their original distributions seen in the previous section other than their scale on\n",
        "the x-axis. We can see that the center of mass for each distribution is centered on zero, which is\n",
        "more obvious for some variables than others."
      ],
      "metadata": {
        "id": "YVUw9uRkfI3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model evaluation\n",
        "Next, let's evaluate the same KNN model as the previous section, but in this case, on a\n",
        "StandardScaler transform of the dataset. The complete example is listed below."
      ],
      "metadata": {
        "id": "MayC0vGAfQGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate knn on the diabetes dataset with standard scaler transform\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define the pipeline\n",
        "trans = StandardScaler()\n",
        "model = KNeighborsClassifier()\n",
        "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
        "# evaluate the pipeline\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report pipeline performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "YDppRTlsfsJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the StandardScaler transform results in a lift in\n",
        "performance from 71.7 percent accuracy without the transform to about 74.1 percent with the\n",
        "transform, slightly higher than the result using the MinMaxScaler that achieved 73.9 percent."
      ],
      "metadata": {
        "id": "U7ywW80kfzxQ"
      }
    }
  ]
}