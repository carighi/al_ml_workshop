{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day_3_Live_Demo_8_Feature_Selection_Recursive_Feature_Elimination.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvXFuipv/nm39mtnNWj7v2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Recursive Feature Elimination**\n","Recursive Feature Elimination, or RFE for short, is a popular feature selection algorithm. RFE\n","is popular because it is easy to configure and use, and effective at selecting those\n","features (columns) in a training dataset that are more or most relevant in predicting the target\n","variable. There are two important configuration options when using RFE: the choice in the\n","number of features to select and the choice of the algorithm used to help choosing the features. Both\n","of these hyperparameters can be explored, although the performance of the method is not\n","strongly dependent on these hyperparameters being configured well.\n","\n","In this tutorial, you will\n","discover how to use Recursive Feature Elimination (RFE) for feature selection in Python. After\n","completing this tutorial, you will know:\n","* RFE is an efficient approach for eliminating features from a training dataset for feature\n","selection.\n","* How to use RFE for feature selection for classification and regression predictive modeling\n","problems.\n","* How to explore the number of selected features and wrapped algorithm used by the RFE\n","procedure.\n","\n","Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."],"metadata":{"id":"HUfyOFwARzpS"}},{"cell_type":"markdown","source":["##RFE for Classification\n","\n","First, we can use the\n","**make_classification**() function to create a synthetic binary classification problem with 1,000\n","examples and 10 input features, five of which are informative and five of which are redundant.\n","\n","Next, we can evaluate an RFE feature selection algorithm on this dataset. We will use a\n","**DecisionTreeClassifier** to choose features and set the number of features to five. We will\n","then fit a new DecisionTreeClassifier model on the selected features. We will evaluate the\n","model using repeated stratified k-fold cross-validation, with three repeats and 10 folds. We will\n","report the mean and standard deviation of the accuracy of the model across all repeats and\n","folds."],"metadata":{"id":"7Q3AZs5LTHR3"}},{"cell_type":"code","source":["# evaluate RFE for classification\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.feature_selection import RFE\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","# define dataset\n","# Generate a random n-class classification problem.\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n","random_state=1)\n","# create pipeline\n","rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n","model = DecisionTreeClassifier()\n","pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n","# evaluate model\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n","# report performance\n","print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDhtL-mDTIUv","executionInfo":{"status":"ok","timestamp":1652899942661,"user_tz":240,"elapsed":1342,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"7621f3c2-a981-4144-d81c-0727b8a9c572"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.885 (0.031)\n"]}]},{"cell_type":"markdown","source":["In this case, we can see the RFE that uses a decision tree and selects five features and\n","then fits a decision tree on the selected features achieves a classification accuracy of about 88\n","percent."],"metadata":{"id":"2yL4Dvh0m0lq"}},{"cell_type":"markdown","source":["We can also use the RFE model pipeline as a final model and make predictions for classification. First, the RFE and model are fit on all available data, then the predict() function can\n","be called to make predictions on new data."],"metadata":{"id":"Oi80hYwMUGjp"}},{"cell_type":"code","source":["# make a prediction with an RFE pipeline\n","from sklearn.datasets import make_classification\n","from sklearn.feature_selection import RFE\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n","random_state=1)\n","# create pipeline\n","rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n","model = DecisionTreeClassifier()\n","pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n","# fit the model on all available data\n","pipeline.fit(X, y)\n","# make a prediction for one example\n","data = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057,\n","-2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n","yhat = pipeline.predict(data)\n","print('Predicted Class: %d' % (yhat))"],"metadata":{"id":"mME-SuNbUKVc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652899977337,"user_tz":240,"elapsed":157,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"eeec9b6a-3bc5-465d-e22f-853aea17ab27"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Class: 1\n"]}]},{"cell_type":"markdown","source":["##RFE for Regression\n","Next, we will look at using RFE for a regression problem. First, we can use the\n","**make_regression**() function to create a synthetic regression problem with 1,000 examples and\n","10 input features, five of which are important and five of which are redundant."],"metadata":{"id":"cNROvvwdURnn"}},{"cell_type":"code","source":["# evaluate RFE for regression\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.feature_selection import RFE\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.pipeline import Pipeline\n","# define dataset\n","# Generate a random regression problem.\n","X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n","# create pipeline\n","rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5)\n","model = DecisionTreeRegressor()\n","pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n","# evaluate model\n","cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","# All scorer objects follow the convention that higher return values are better than lower return values. \n","# Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, \n","# are available as neg_mean_squared_error which return the negated value of the metric.\n","n_scores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv,\n","n_jobs=-1)\n","# report performance\n","print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vyHx4ph6UusV","executionInfo":{"status":"ok","timestamp":1652900353635,"user_tz":240,"elapsed":1348,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"9d7f1ac4-5942-4d68-e329-9ca0a7f297e9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["MAE: -27.373 (2.966)\n"]}]},{"cell_type":"markdown","source":["In this case, we can see the RFE pipeline with a decision tree model achieves a MAE of\n","about 27."],"metadata":{"id":"YXE8ErMym_im"}},{"cell_type":"markdown","source":["We can also use the RFE as part of the final model and make predictions for regression.\n","First, the Pipeline is fit on all available data, then the predict() function can be called to\n","make predictions on new data."],"metadata":{"id":"FiLHrPerU2gY"}},{"cell_type":"code","source":["# make a regression prediction with an RFE pipeline\n","from sklearn.datasets import make_regression\n","from sklearn.feature_selection import RFE\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.pipeline import Pipeline\n","# define dataset\n","X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n","# create pipeline\n","rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5)\n","model = DecisionTreeRegressor()\n","pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n","# fit the model on all available data\n","pipeline.fit(X, y)\n","# make a prediction for one example\n","data = [[-2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381,\n","0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n","yhat = pipeline.predict(data)\n","print('Predicted: %.3f' % (yhat))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2sbjOapU3T0","executionInfo":{"status":"ok","timestamp":1652900372038,"user_tz":240,"elapsed":150,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"a165fd5e-daf7-4b25-a620-8391bfbb2e26"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted: -84.288\n"]}]},{"cell_type":"markdown","source":["##RFE Hyperparameters\n","\n","In this section, we will take a closer look at some of the hyperparameters you should consider\n","tuning for the RFE method for feature selection and their effect on model performance."],"metadata":{"id":"PesWYn4hVVoq"}},{"cell_type":"markdown","source":["###Explore Number of Features\n","An important hyperparameter for the RFE algorithm is the number of features to select. In\n","the previous section, we used an arbitrary number of selected features, five, which matches\n","the number of informative features in the synthetic dataset. In practice, we cannot know the\n","best number of features to select with RFE; instead, it is good practice to test different values."],"metadata":{"id":"zsKjF6nFVos3"}},{"cell_type":"code","source":["# explore the number of selected features for RFE\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.feature_selection import RFE\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","from matplotlib import pyplot\n","# get the dataset\n","def get_dataset():\n","  X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n","  random_state=1)\n","  return X, y\n","\n","# get a list of models to evaluate\n","def get_models():\n","  models = dict()\n","  for i in range(2, 10):\n","    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n","    model = DecisionTreeClassifier()\n","    models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n","  return models\n","\n","# evaluate a given model using cross-validation\n","def evaluate_model(model, X, y):\n","  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","  scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n","  return scores\n","  \n","# define dataset\n","X, y = get_dataset()\n","# get the models to evaluate\n","models = get_models()\n","# evaluate the models and store results\n","results, names = list(), list()\n","for name, model in models.items():\n","  scores = evaluate_model(model, X, y)\n","  results.append(scores)\n","  names.append(name)\n","  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvducXVzVtwN","executionInfo":{"status":"ok","timestamp":1652900435063,"user_tz":240,"elapsed":11207,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"131d2018-b9b6-4976-df87-6fd2c9cb7cd2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":[">2 0.717 (0.046)\n",">3 0.827 (0.031)\n",">4 0.872 (0.031)\n",">5 0.884 (0.033)\n",">6 0.891 (0.027)\n",">7 0.887 (0.028)\n",">8 0.884 (0.028)\n",">9 0.887 (0.027)\n"]}]},{"cell_type":"markdown","source":["In this case, we can see that performance improves as the number of features increase and\n","perhaps peaks around 4-to-7 as we might expect, given that only  five features are relevant to\n","the target variable."],"metadata":{"id":"QbDUiTKqWCa5"}},{"cell_type":"markdown","source":["A box and whisker plot is created for the distribution of accuracy scores for each con gured\n","number of features."],"metadata":{"id":"6ETraJSWWJPP"}},{"cell_type":"code","source":["# plot model performance for comparison\n","pyplot.boxplot(results, labels=names, showmeans=True)\n","pyplot.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"pYNP4wlsWMJe","executionInfo":{"status":"ok","timestamp":1652900448939,"user_tz":240,"elapsed":326,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"15fb9d2b-6e3a-422a-ec75-5441f4beb70e"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["###Automatically Select the Number of Features\n","\n","It is also possible to automatically select the number of features chosen by RFE. This can be\n","achieved by performing cross-validation evaluation of different numbers of features as we did in\n","the previous section and automatically selecting the number of features that resulted in the\n","best mean score. The **RFECV** class implements this."],"metadata":{"id":"7FoMLzw6nMs5"}},{"cell_type":"code","source":["# automatically select the number of features for RFE\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.feature_selection import RFECV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n","random_state=1)\n","# create pipeline\n","# Recursive feature elimination with cross-validation to select the number of features.\n","rfe = RFECV(estimator=DecisionTreeClassifier())\n","model = DecisionTreeClassifier()\n","pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n","# evaluate model\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n","# report performance\n","print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmrvLUpcnVc-","executionInfo":{"status":"ok","timestamp":1652900529102,"user_tz":240,"elapsed":5741,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"47e84f02-50f7-45b6-b658-3b109b91b43f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.882 (0.024)\n"]}]},{"cell_type":"markdown","source":["In this case, we can see the RFE that uses a decision tree and automatically selects a number\n","of features and then fits a decision tree on the selected features achieves a classification accuracy\n","of about 88 percent."],"metadata":{"id":"hbJmN8dznZXi"}},{"cell_type":"markdown","source":["###Which Features Were Selected\n","When using RFE, we may be interested to know which features were selected and which were\n","removed. This can be achieved by reviewing the attributes of the  fit **RFE** object (or  fit **RFECV**\n","object). The support attribute reports true or false as to which features in order of column\n","index were included and the ranking attribute reports the relative ranking of features in the\n","same order. The example below fits an RFE model on the whole dataset and selects five features,\n","then reports each feature column index (0 to 9), whether it was selected or not (True or False),\n","and the relative feature ranking."],"metadata":{"id":"HvKdFwzrnebK"}},{"cell_type":"code","source":["# report which features were selected by RFE\n","from sklearn.datasets import make_classification\n","from sklearn.feature_selection import RFE\n","from sklearn.tree import DecisionTreeClassifier\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n","random_state=1)\n","# define RFE\n","# Feature ranking with recursive feature elimination.\n","rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n","# fit RFE\n","rfe.fit(X, y)\n","# summarize all features\n","for i in range(X.shape[1]):\n","  print('Column: %d, Selected=%s, Rank: %d' % (i, rfe.support_[i], rfe.ranking_[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhAJli-znlEp","executionInfo":{"status":"ok","timestamp":1652900570599,"user_tz":240,"elapsed":168,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"eb7a0d7e-c6fb-4d39-abf0-dba446fa497b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Column: 0, Selected=False, Rank: 4\n","Column: 1, Selected=False, Rank: 5\n","Column: 2, Selected=True, Rank: 1\n","Column: 3, Selected=True, Rank: 1\n","Column: 4, Selected=True, Rank: 1\n","Column: 5, Selected=False, Rank: 6\n","Column: 6, Selected=True, Rank: 1\n","Column: 7, Selected=False, Rank: 3\n","Column: 8, Selected=True, Rank: 1\n","Column: 9, Selected=False, Rank: 2\n"]}]},{"cell_type":"markdown","source":["###Explore Estimator Algorithm\n","There are many algorithms that can be used in the core RFE, as long as they provide some\n","indication of variable importance. Most decision tree algorithms are likely to report the same\n","general trends in feature importance, but this is not guaranteed. It might be helpful to explore\n","the use of different algorithms wrapped by RFE. The example below demonstrates how you\n","might explore this configuration option."],"metadata":{"id":"yVyNprsjnyZh"}},{"cell_type":"code","source":["# explore the algorithm wrapped by RFE\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import Perceptron\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.pipeline import Pipeline\n","from matplotlib import pyplot\n","# get the dataset\n","def get_dataset():\n","  X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n","  random_state=1)\n","  return X, y\n","\n","# get a list of models to evaluate\n","def get_models():\n","  models = dict()\n","\n","  # lr - LogisticRegression\n","  # estimator - A supervised learning estimator with a fit method that provides information about feature importance\n","  rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\n","  model = DecisionTreeClassifier()\n","  models['lr'] = Pipeline(steps=[('s',rfe),('m',model)])\n","  # per - Perceptron\n","  rfe = RFE(estimator=Perceptron(), n_features_to_select=5)\n","  model = DecisionTreeClassifier()\n","  models['per'] = Pipeline(steps=[('s',rfe),('m',model)])\n","  # dtc - DecisionTreeClassifier\n","  rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n","  model = DecisionTreeClassifier()\n","  models['dtc'] = Pipeline(steps=[('s',rfe),('m',model)])\n","  # rf - RandomForestClassifier\n","  rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5)\n","  model = DecisionTreeClassifier()\n","  models['rf'] = Pipeline(steps=[('s',rfe),('m',model)])\n","  # gbm - GradientBoostingClassifier\n","  rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=5)\n","  model = DecisionTreeClassifier()\n","  models['gbm'] = Pipeline(steps=[('s',rfe),('m',model)])\n","  return models\n","\n","# evaluate a given model using cross-validation\n","def evaluate_model(model, X, y):\n","  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","  scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n","  return scores\n","# define dataset\n","X, y = get_dataset()\n","# get the models to evaluate\n","models = get_models()\n","# evaluate the models and store results\n","results, names = list(), list()\n","for name, model in models.items():\n","  scores = evaluate_model(model, X, y)\n","  results.append(scores)\n","  names.append(name)\n","  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxXFnbbXn0Vu","executionInfo":{"status":"ok","timestamp":1652900915460,"user_tz":240,"elapsed":81488,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"34b8cd02-f408-4c14-a1fc-acb4a7429c30"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":[">lr 0.889 (0.032)\n",">per 0.847 (0.039)\n",">dtc 0.882 (0.030)\n",">rf 0.852 (0.042)\n",">gbm 0.888 (0.031)\n"]}]},{"cell_type":"markdown","source":["In this case, the results suggest that linear algorithms like logistic regression might select better features more reliably than the chosen decision tree and ensemble\n","of decision tree algorithms."],"metadata":{"id":"Zl8US9ntoUWi"}},{"cell_type":"code","source":["# plot model performance for comparison\n","pyplot.boxplot(results, labels=names, showmeans=True)\n","pyplot.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"rM3X95TsoVM3","executionInfo":{"status":"ok","timestamp":1652900930410,"user_tz":240,"elapsed":359,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"bc31ea52-36c1-493f-c03e-1240c1244e2f"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["A box and whisker plot is created for the distribution of accuracy scores for each configured\n","wrapped algorithm. We can see the general trend of good performance with logistic regression,\n","DTC and perhaps GBM. The model used within RFE can make an important\n","difference to which features are selected and in turn the performance on the prediction problem."],"metadata":{"id":"OxqN0nvJobwo"}}]}