{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Feature_Selection_Numerical_Input_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Select Numerical Input Features**"
      ],
      "metadata": {
        "id": "3YRTywaATcN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is Feature Selection?\n",
        "\n",
        "Feature selection is the process of identifying and selecting a subset of input features that are\n",
        "most relevant to the target variable. Feature selection is often straightforward when working\n",
        "with real-valued input and output data, such as using the Pearson's correlation coefficient, but\n",
        "can be challenging when working with numerical input data and a categorical target variable.\n",
        "The two most commonly used feature selection methods for numerical input data when the\n",
        "target variable is categorical (e.g. classification predictive modeling) are the ANOVA F-test\n",
        "statistic and the mutual information statistic.\n",
        "\n",
        "In this tutorial, you will discover how to perform\n",
        "feature selection with numerical input data for classification. After completing this tutorial, you\n",
        "will know:\n",
        "* The diabetes predictive modeling problem with numerical inputs and binary classification\n",
        "target variables.\n",
        "* How to evaluate the importance of numerical features using the ANOVA F-test and mutual\n",
        "information statistics.\n",
        "* How to perform feature selection for numerical data when  tting and evaluating a classification model.\n",
        "\n",
        "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
      ],
      "metadata": {
        "id": "2_FqA3LR-fSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Diabetes Dataset\n",
        "The dataset classifies patient data as\n",
        "either an onset of diabetes within five years or not. There are 768 examples and eight input variables.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "\n",
        "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
        "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))\n",
        "\n",
        "##Download Diabetes data files"
      ],
      "metadata": {
        "id": "WWCBwHWd9qF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -O pima-indians-diabetes.csv\n",
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -O pima-indians-diabetes.names\n",
        "!head pima-indians-diabetes.csv"
      ],
      "metadata": {
        "id": "PuraTCQX-BiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and splitting Diabetes data"
      ],
      "metadata": {
        "id": "1RM6_F6B-MMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and summarize the dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# load the dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# summarize\n",
        "print('Train', X_train.shape, y_train.shape)\n",
        "print('Test', X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "ilnteuT8-Rrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numerical Feature Selection\n",
        "There are two popular feature selection techniques that can be used for numerical input data\n",
        "and a categorical (class) target variable. They are:\n",
        "* ANOVA F-Statistic.\n",
        "* Mutual Information Statistics.\n",
        "\n"
      ],
      "metadata": {
        "id": "m4qhNNtX-iPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ANOVA F-test Feature Selection**\n",
        "ANOVA is an acronym for analysis of variance and is a parametric statistical hypothesis test for\n",
        "determining whether the means from two or more samples of data (often three or more) come\n",
        "from the same distribution or not. An F-statistic, or F-test, is a class of statistical tests that\n",
        "calculate the ratio between variances values, such as the variance from two different samples or\n",
        "the explained and unexplained variance by a statistical test, like ANOVA. The ANOVA method\n",
        "is a type of F-statistic referred to here as an ANOVA F-test.\n",
        "\n",
        "Importantly, ANOVA is used when one variable is numeric and one is categorical, such as\n",
        "numerical input variables and a classification target variable in a classification task. The results\n",
        "of this test can be used for feature selection where those features that are independent of the\n",
        "target variable can be removed from the dataset."
      ],
      "metadata": {
        "id": "sf57AM1pJ6ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of anova f-test feature selection for numerical data\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from matplotlib import pyplot\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# feature selection\n",
        "def select_features(X_train, y_train, X_test):\n",
        "  # configure to select all features\n",
        "  fs = SelectKBest(score_func=f_classif, k='all')\n",
        "  # learn relationship from training data\n",
        "  fs.fit(X_train, y_train)\n",
        "  # transform train input data\n",
        "  X_train_fs = fs.transform(X_train)\n",
        "  # transform test input data\n",
        "  X_test_fs = fs.transform(X_test)\n",
        "  return X_train_fs, X_test_fs, fs\n",
        "\n",
        "# load the dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# feature selection\n",
        "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
        "\n",
        "# what are scores for the features\n",
        "for i in range(len(fs.scores_)):\n",
        "  print('Feature %d: %f' % (i, fs.scores_[i]))\n"
      ],
      "metadata": {
        "id": "vc2r-_NF-6I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can see that some features stand out as perhaps being more relevant than\n",
        "others, with much larger test statistic values. Perhaps features 1, 5, and 7 are most relevant."
      ],
      "metadata": {
        "id": "s6kBx3Nxi7db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the scores\n",
        "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "-Pz-EHTHi9o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart of the feature importance scores for each input feature is created. This clearly\n",
        "shows that feature 1 might be the most relevant (according to test statistic) and that perhaps\n",
        "six of the eight input features are the most relevant. We could set k=6 when configuring the\n",
        "SelectKBest to select these six features."
      ],
      "metadata": {
        "id": "oKSgv_qgjHxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Mutual Information Feature Selection**\n",
        "Mutual information from the field of information theory is the application of information gain\n",
        "(typically used in the construction of decision trees) to feature selection. Mutual information is\n",
        "calculated between two variables and measures the reduction in uncertainty for one variable given\n",
        "a known value of the other variable. Mutual information is straightforward when considering\n",
        "the distribution of two discrete (categorical or ordinal) variables, such as categorical input and\n",
        "categorical output data. Nevertheless, it can be adapted for use with numerical input and\n",
        "categorical output."
      ],
      "metadata": {
        "id": "-1C391xu_n9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of mutual information feature selection for numerical input data\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from matplotlib import pyplot\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# feature selection\n",
        "def select_features(X_train, y_train, X_test):\n",
        "  # configure to select all features\n",
        "  fs = SelectKBest(score_func=mutual_info_classif, k='all')\n",
        "  # learn relationship from training data\n",
        "  fs.fit(X_train, y_train)\n",
        "  # transform train input data\n",
        "  X_train_fs = fs.transform(X_train)\n",
        "  # transform test input data\n",
        "  X_test_fs = fs.transform(X_test)\n",
        "  return X_train_fs, X_test_fs, fs\n",
        "\n",
        "# load the dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# feature selection\n",
        "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
        "# what are scores for the features\n",
        "for i in range(len(fs.scores_)):\n",
        "  print('Feature %d: %f' % (i, fs.scores_[i]))\n"
      ],
      "metadata": {
        "id": "mmGSRrjlIvwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can see that some of the features have a modestly low score, suggesting that\n",
        "perhaps they can be removed. Perhaps features 1 and 5 are most relevant."
      ],
      "metadata": {
        "id": "EnIkfo4TjOC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the scores\n",
        "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "oZ_FSeTkjQpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart of the feature importance scores for each input feature is created. Importantly,\n",
        "a different mixture of features is promoted."
      ],
      "metadata": {
        "id": "jbbyDlxHjYWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modeling With Selected Features\n",
        " A robust approach is to evaluate models using different\n",
        "feature selection methods (and numbers of features) and select the method that results in a\n",
        "model with the best performance."
      ],
      "metadata": {
        "id": "Tnan388lKE4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Built Using All Features**"
      ],
      "metadata": {
        "id": "AnmcLcQuKzJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation of a model using all input features\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# load the dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# fit the model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "HngJ41InKL1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can see that the model achieves a classification accuracy of about 77 percent.\n",
        "We would prefer to use a subset of features that achieves a classification accuracy that is as\n",
        "good or better than this."
      ],
      "metadata": {
        "id": "wx2viBcvkpim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Built Using ANOVA F-test Features**\n",
        "We can use the ANOVA F-test to score the features and select the four most relevant features."
      ],
      "metadata": {
        "id": "IwqLyIQwKS7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation of a model using 4 features chosen with anova f-test\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# feature selection\n",
        "def select_features(X_train, y_train, X_test):\n",
        "  # configure to select a subset of features\n",
        "  fs = SelectKBest(score_func=f_classif, k=4)\n",
        "  # learn relationship from training data\n",
        "  fs.fit(X_train, y_train)\n",
        "  # transform train input data\n",
        "  X_train_fs = fs.transform(X_train)\n",
        "  # transform test input data\n",
        "  X_test_fs = fs.transform(X_test)\n",
        "  return X_train_fs, X_test_fs, fs\n",
        "\n",
        "# load the dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# feature selection\n",
        "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
        "# fit the model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_fs, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test_fs)\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "RAsuuSwsKUmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we see that the model achieved an accuracy of about 78.74 percent, a lift in\n",
        "performance compared to the baseline that achieved 77.56 percent."
      ],
      "metadata": {
        "id": "7tzXpJAJkuEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Built Using Mutual Information Features**\n",
        "We can repeat the experiment and select the top four features using a mutual information\n",
        "statistic."
      ],
      "metadata": {
        "id": "_Imiv_JfK4DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation of a model using 4 features chosen with mutual information\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# feature selection\n",
        "def select_features(X_train, y_train, X_test):\n",
        "  # configure to select a subset of features\n",
        "  fs = SelectKBest(score_func=mutual_info_classif, k=4) # mutual_info_classif()\n",
        "  # learn relationship from training data\n",
        "  fs.fit(X_train, y_train)\n",
        "  # transform train input data\n",
        "  X_train_fs = fs.transform(X_train)\n",
        "  # transform test input data\n",
        "  X_test_fs = fs.transform(X_test)\n",
        "  return X_train_fs, X_test_fs, fs\n",
        "\n",
        "# load the dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# feature selection\n",
        "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
        "# fit the model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_fs, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test_fs)\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "urhebigRLEuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can make no difference compared to the baseline model. This is interesting\n",
        "as we know the method chose a different four features compared to the previous method."
      ],
      "metadata": {
        "id": "A-YjNZrNkzk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tune the Number of Selected Features\n",
        "Instead of guessing, we can systematically test a range\n",
        "of different numbers of selected features and discover which results in the best performing\n",
        "model. This is called a **grid search**, where the k argument to the SelectKBest class can be\n",
        "tuned. It is good practice to evaluate model configurations on classification tasks using repeated\n",
        "stratified k-fold cross-validation. We will use three repeats of 10-fold cross-validation via the\n",
        "RepeatedStratifiedKFold class."
      ],
      "metadata": {
        "id": "4z9yH_7OaXhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare different numbers of features selected using anova f-test\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# define dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# define the evaluation method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define the pipeline to evaluate\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "fs = SelectKBest(score_func=f_classif)\n",
        "pipeline = Pipeline(steps=[('anova',fs), ('lr', model)])\n",
        "# define the grid\n",
        "grid = dict()\n",
        "grid['anova__k'] = [i+1 for i in range(X.shape[1])]\n",
        "print(grid)\n",
        "# define the grid search\n",
        "search = GridSearchCV(pipeline, grid, scoring='accuracy', n_jobs=-1, cv=cv)\n",
        "# perform the search\n",
        "results = search.fit(X, y)\n",
        "# summarize best\n",
        "print('Best Mean Accuracy: %.3f' % results.best_score_)\n",
        "print('Best Config: %s' % results.best_params_)"
      ],
      "metadata": {
        "id": "Yj-ukFvValmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can see that the best number of selected features is five; that achieves an\n",
        "accuracy of about 77 percent."
      ],
      "metadata": {
        "id": "jqldqJg8k-OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might want to see the relationship between the number of selected features and classification accuracy. In this relationship, we may expect that more features result in a better\n",
        "performance to a point. This relationship can be explored by manually evaluating each configuration of **k** for the SelectKBest, gathering the sample of accuracy scores, and\n",
        "plotting the results using box and whisker plots side-by-side."
      ],
      "metadata": {
        "id": "KvMyK8MNdtk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare different numbers of features selected using anova f-test\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "  # load the dataset as a pandas DataFrame\n",
        "  data = read_csv(filename, header=None)\n",
        "  # retrieve numpy array\n",
        "  dataset = data.values\n",
        "  # split into input (X) and output (y) variables\n",
        "  X = dataset[:, :-1]\n",
        "  y = dataset[:,-1]\n",
        "  return X, y\n",
        "\n",
        "# evaluate a given model using cross-validation\n",
        "def evaluate_model(model):\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  return scores\n",
        "# define dataset\n",
        "X, y = load_dataset('pima-indians-diabetes.csv')\n",
        "# define number of features to evaluate\n",
        "num_features = [i+1 for i in range(X.shape[1])]\n",
        "# enumerate each number of features\n",
        "results = list()\n",
        "for k in num_features:\n",
        "  # create pipeline\n",
        "  model = LogisticRegression(solver='liblinear')\n",
        "  fs = SelectKBest(score_func=f_classif, k=k)\n",
        "  pipeline = Pipeline(steps=[('anova',fs), ('lr', model)])\n",
        "  # evaluate the model\n",
        "  scores = evaluate_model(pipeline)\n",
        "  results.append(scores)\n",
        "  # summarize the results\n",
        "  print('>%d %.3f (%.3f)' % (k, mean(scores), std(scores)))\n"
      ],
      "metadata": {
        "id": "f4eyOoXidvkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, it looks like selecting five or seven features results in roughly the same accuracy."
      ],
      "metadata": {
        "id": "XVf35NWxlEBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, tick_labels=num_features, showmeans=True)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "oF3Fc2VVlF3G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}