{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Outlier_Identification_and_Removal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XaIh9kpwKe-"
      },
      "source": [
        "#**Outlier Identification and Removal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6QcSd0aTFwl"
      },
      "source": [
        "In this tutorial, you will learn:\n",
        "\n",
        "* That an outlier is an unlikely observation in a dataset and may have one of many causes.\n",
        "* How to use simple univariate statistics like standard deviation and interquartile range to\n",
        "identify and remove outliers from a data sample.\n",
        "* How to use an outlier detection model to identify and remove rows from a training dataset\n",
        "in order to lift predictive modeling performance.\n",
        "\n",
        "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
        "\n",
        "Code explanations are done with aid of CS50.ai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GhsEsk5Qbrh"
      },
      "source": [
        "#Outlier Identification and Removal\n",
        "##What are Outliers?\n",
        "\n",
        "An outlier is an observation that is unlike the other observations. They are rare, distinct, or do\n",
        "not fit in some way.\n",
        "\n",
        "We will generally define outliers as samples that are exceptionally far from the\n",
        "mainstream of the data.\n",
        "\n",
        "Outliers can have many causes, such as:\n",
        "\n",
        "* Measurement or input error.\n",
        "\n",
        "* Data corruption.\n",
        "\n",
        "* True outlier observation.\n",
        "\n",
        "There is no precise way to define and identify outliers in general because of the specifics of\n",
        "each dataset. Instead, you, or a domain expert, must interpret the raw observations and decide\n",
        "whether a value is an outlier or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfT4GpLuQ_qd"
      },
      "source": [
        "##Remove outliers using Standard Deviation method"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will generate a set of random numbers and calculate the mean and standard deviation.\n",
        "The code below does the following:\n",
        "\n",
        "    It imports necessary functions from the numpy library.\n",
        "    It sets a seed for the random number generator. This is done to ensure that the same set of random numbers is generated each time the code is run.\n",
        "    It generates 10,000 random numbers that follow a normal (Gaussian) distribution. The randn(10000) function generates these numbers with a mean of 0 and a standard deviation of 1. Multiplying by 5 and adding 50 shifts this distribution to have a mean of 50 and a standard deviation of 5.\n",
        "    It calculates the mean and standard deviation of these numbers using the mean(data) and std(data) functions.\n",
        "    It prints the mean and standard deviation of the generated data. The %.3f is a placeholder for a floating-point number, with 3 digits after the decimal point. The % operator is then used to insert the mean and standard deviation into these placeholders.\n"
      ],
      "metadata": {
        "id": "_1GoxUbIKcoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKh3tdwURBl9"
      },
      "outputs": [],
      "source": [
        "# identifiy outliers using Standard Deviation method\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate univariate observations\n",
        "data = 5 * randn(10000) + 50\n",
        "# summarize\n",
        "print('mean=%.3f stdv=%.3f' % (mean(data), std(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yirudZwiRagS"
      },
      "outputs": [],
      "source": [
        "# calculate summary statistics\n",
        "data_mean, data_std = mean(data), std(data)\n",
        "# define outliers. Explain code below\n",
        "cut_off = data_std * 3\n",
        "lower, upper = data_mean - cut_off, data_mean + cut_off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6m6_COPRevk"
      },
      "outputs": [],
      "source": [
        "# identify outliers\n",
        "outliers = [x for x in data if x < lower or x > upper]\n",
        "print('Identified outliers: %d' % len(outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emxpuI6XRxtl"
      },
      "outputs": [],
      "source": [
        "# remove outliers\n",
        "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
        "print('Non-outlier observations: %d' % len(outliers_removed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKbBHWIdTtXl"
      },
      "source": [
        "##Remove outliers using Interquartile Range method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeFKqZG-T7Cn"
      },
      "outputs": [],
      "source": [
        "# identify outliers with interquartile range\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import percentile\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate univariate observations\n",
        "data = 5 * randn(10000) + 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsHxjzHhT-QS"
      },
      "outputs": [],
      "source": [
        "# calculate interquartile range\n",
        "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
        "iqr = q75 - q25\n",
        "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P26z_D9kUAyI"
      },
      "outputs": [],
      "source": [
        "# calculate the outlier cutoff\n",
        "cut_off = iqr * 1.5\n",
        "lower, upper = q25 - cut_off, q75 + cut_off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ygpNxrRUDDB"
      },
      "outputs": [],
      "source": [
        "# identify outliers\n",
        "outliers = [x for x in data if x < lower or x > upper]\n",
        "print('Identified outliers: %d' % len(outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hca_q8jxUHFq"
      },
      "outputs": [],
      "source": [
        "# remove outliers\n",
        "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
        "print('Non-outlier observations: %d' % len(outliers_removed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KInzCDfkYPRq"
      },
      "source": [
        "##Remove outliers using Automatic Outlier Detection method\n",
        "\n",
        "A simple approach to identifying outliers is to locate those examples that are far from the\n",
        "other examples in the multi-dimensional feature space. This can work well for feature spaces\n",
        "with low dimensionality (few features), although it can become less reliable as the number of\n",
        "features is increased, referred to as the **curse of dimensionality**. The local outlier factor, or\n",
        "LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier\n",
        "detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers\n",
        "based on the size of its local neighborhood. Those examples with the largest score are more\n",
        "likely to be outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxaSOQTEYpYA"
      },
      "source": [
        "##Diabetes Dataset\n",
        "The dataset classifies patient data as\n",
        "either an onset of diabetes within five years or not. There are 768 examples and eight input variables. It is a binary classification problem.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "\n",
        "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
        "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSk8mA5LY9LG"
      },
      "source": [
        "###Download Diabetes data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpRFKdgtZAxn"
      },
      "outputs": [],
      "source": [
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -O pima-indians-diabetes.csv\n",
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -O pima-indians-diabetes.names\n",
        "!head pima-indians-diabetes.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code does the following steps:\n",
        "\n",
        "    It imports necessary functions from the pandas and scikit-learn libraries.\n",
        "    It loads the dataset from a CSV file using pandas' read_csv function.\n",
        "    It converts the DataFrame to a numpy array using the values attribute.\n",
        "    It separates the dataset into input features (X) and the target variable (y) using numpy slicing.\n",
        "    It prints the shapes of X and y to give an idea of the dataset's size.\n",
        "    It splits the dataset into training and test sets using the train_test_split function. 70% of the data is used for training and 30% for testing.\n",
        "    Finally, it prints the shapes of the training and test sets to confirm the split.\n"
      ],
      "metadata": {
        "id": "eWSsGSpNMiiR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxKqHxWoZ7ji"
      },
      "outputs": [],
      "source": [
        "# load and summarize the dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "# load the dataset\n",
        "df = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve the array\n",
        "data = df.values\n",
        "# split into input and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# summarize the shape of the dataset\n",
        "print(X.shape, y.shape)\n",
        "# split into train (70%) and test sets (30%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "# summarize the shape of the train and test sets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following from last step above (splitting the dataset into training and test sets using the train_test_split function), here are the additional steps:\n",
        "  \n",
        "    It initializes a Linear Regression model and fits it on the training data using the fit method.\n",
        "    It makes predictions on the test data using the predict method.\n",
        "    It calculates the MAE between the actual and predicted values using the mean_absolute_error function. MAE is a measure of prediction error that calculates the average absolute difference between actual and predicted values.\n",
        "    It prints the MAE. The %.3f is a placeholder for a floating-point number, with 3 digits after the decimal point. The % operator is then used to insert the MAE into this placeholder.\n"
      ],
      "metadata": {
        "id": "Aky_KfC-NGp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqUE_CgradMj"
      },
      "outputs": [],
      "source": [
        "# evaluate model on the raw dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# load the dataset\n",
        "df = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve the array\n",
        "data = df.values\n",
        "# split into input and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "# fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions using mean absolute error\n",
        "mae = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlqg7DbKar1a"
      },
      "source": [
        "Next, we can try removing outliers from the training dataset. The expectation is that the\n",
        "outliers are causing the linear regression model to learn a bias or skewed understanding of the\n",
        "problem, and that removing these outliers from the training set will allow a more e ective model\n",
        "to be learned.\n",
        "\n",
        "We can achieve this by defining the **LocalOutlierFactor** model and using it to\n",
        "make a prediction on the training dataset, marking each row in the training dataset as normal\n",
        "(1) or an outlier (-1). We will use the default hyperparameters for the outlier detection model,\n",
        "although it is a good idea to tune the configuration to the specifics of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQfe83Tnasqz"
      },
      "outputs": [],
      "source": [
        "# evaluate model on training dataset with outliers removed\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# load the dataset\n",
        "df = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve the array\n",
        "data = df.values\n",
        "# split into input and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "# summarize the shape of the training dataset\n",
        "print(X_train.shape, y_train.shape)\n",
        "# identify outliers in the training dataset\n",
        "lof = LocalOutlierFactor()\n",
        "yhat = lof.fit_predict(X_train)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "X_train, y_train = X_train[mask, :], y_train[mask]\n",
        "# summarize the shape of the updated training dataset\n",
        "print(X_train.shape, y_train.shape)\n",
        "# fit the model without outliers\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "mae = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see MAE (Mean Absolute Error) reduced from to 0.324 0.317."
      ],
      "metadata": {
        "id": "_qm9qvTDMbCY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}