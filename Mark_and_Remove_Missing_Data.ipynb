{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Mark_and_Remove_Missing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mark and Remove Missing Data**"
      ],
      "metadata": {
        "id": "BiXdwSXiwRFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, you will learn:\n",
        "\n",
        "* How to mark invalid or corrupt values as missing in your dataset.\n",
        "* How to confirm that the presence of marked missing values causes problems for learning algorithms.\n",
        "* How to remove rows with missing data from your dataset and evaluate a learning algorithm on the transformed dataset.\n",
        "\n",
        "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
      ],
      "metadata": {
        "id": "BLGTNOxtV9mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Diabetes Dataset\n",
        "The dataset classifies patient data as\n",
        "either an onset of diabetes withinâ€€five years or not. There are 768 examples and eight input variables. It is a binary classification problem.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "\n",
        "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
        "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))\n",
        "\n",
        "The description of Diabetes Dataset can be found [here](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)."
      ],
      "metadata": {
        "id": "yUN0bWQCtwyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download Diabetes data files"
      ],
      "metadata": {
        "id": "Q6sV7U6lubK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -O pima-indians-diabetes.csv\n",
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -O pima-indians-diabetes.names\n",
        "!head pima-indians-diabetes.csv"
      ],
      "metadata": {
        "id": "d0P3MkJSuf5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and summarize the dataset\n",
        "import pandas as pd\n",
        "# load the dataset\n",
        "dataset = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
        "dataset.columns = ['Number of times pregnant', 'Plasma glucose concentration', 'Diastolic blood pressure', 'Triceps skinfold thickness', '2-Hour serum insulin', 'Body mass index', 'Diabetes pedigree function', 'Age', 'Class variable (0 or 1)']\n",
        "# summarize the dataset (Hint: .describe())\n",
        "#Your code here"
      ],
      "metadata": {
        "id": "uN4dFedLuxWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are columns that have a minimum value of zero (0).\n",
        "On some columns, a value of zero does not make sense and indicates an invalid or missing value."
      ],
      "metadata": {
        "id": "mvxFMHDsu8AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifically, the following columns have an invalid zero minimum value:\n",
        "1. Plasma glucose concentration\n",
        "1. Diastolic blood pressure\n",
        "1. Triceps skinfold thickness\n",
        "1. 2-Hour serum insulin\n",
        "1. Body mass index\n",
        "\n",
        "We can confirm this by looking at the raw data and printing out the first 20 rows of data."
      ],
      "metadata": {
        "id": "z4bgVvb7u_Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the first 20 rows of data\n",
        "# your code here"
      ],
      "metadata": {
        "id": "2oz8iUJHvU4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get a count of the number of missing values on each of these columns."
      ],
      "metadata": {
        "id": "zxBY7SILvhCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarizing the number of missing values for each variable\n",
        "# count the number of missing values for each column\n",
        "num_missing = (dataset[['Plasma glucose concentration', 'Diastolic blood pressure', 'Triceps skinfold thickness', '2-Hour serum insulin', 'Body mass index']] == 0).sum()\n",
        "# report the results\n",
        "print(num_missing)"
      ],
      "metadata": {
        "id": "q8J4dzTtvh0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that columns indexed 1, 2 and 5 have just a few zero values, whereas columns 3 and 4\n",
        "show a lot more, nearly half of the rows. This highlights that different missing value strategies\n",
        "may be needed for different columns, e.g. to ensure that there are still a sufficient number of\n",
        "records left to train a predictive model."
      ],
      "metadata": {
        "id": "nqNGybzwvp5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, specifically Pandas, NumPy and Scikit-Learn, we mark missing values as NaN.\n",
        "Values with a NaN value are ignored from operations like sum, count, etc. We can mark values\n",
        "as NaN easily with the Pandas DataFrame by using the replace() function on a subset of\n",
        "the columns we are interested in. After we have marked the missing values, we can use the\n",
        "isnull() function to mark all of the NaN values in the dataset as True and get a count of the\n",
        "missing values for each column."
      ],
      "metadata": {
        "id": "OOMxI86jws8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Marking missing values with nan values\n",
        "from numpy import nan\n",
        "from pandas import read_csv\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# replace '0' values with 'nan'\n",
        "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
        "# count the number of nan values in each column. Hint: use .isnull().sum()\n",
        "# your code here"
      ],
      "metadata": {
        "id": "uU-TQ-Ahwt2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can confirm by printing out the first 20 rows of data."
      ],
      "metadata": {
        "id": "XYVSPWK_xJBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Review data with missing values marked with a nan\n",
        "from numpy import nan\n",
        "from pandas import read_csv\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# replace '0' values with 'nan'\n",
        "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
        "# summarize the first 20 rows of data\n",
        "# your code here"
      ],
      "metadata": {
        "id": "hXmUyI0FxNPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Missing Values Cause Problems\n",
        "Having missing values in a dataset can cause errors with some machine learning algorithms. We are going to try classification of diabetes vs non diabetes using Linear Discrimant Analysis or LDA. LDA is a technique used for classification tasks. LDA is a classification algorithm that finds a linear combination of features that characterizes or separates two or more classes of objects or events. It assumes that the input variables have a Gaussian distribution and the same variance. This algorithm is sensitive to missing data.\n",
        "When you run the next cell, you will get an error because of this."
      ],
      "metadata": {
        "id": "f-m_mOX9xV8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example where missing values cause errors\n",
        "from numpy import nan\n",
        "from pandas import read_csv\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# replace '0' values with 'nan'\n",
        "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
        "# split dataset into inputs and outputs\n",
        "values = dataset.values\n",
        "X = values[:,0:8]\n",
        "y = values[:,8]\n",
        "# define the model\n",
        "model = LinearDiscriminantAnalysis()\n",
        "# define the model evaluation procedure using K fold cross-valiation\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "# evaluate the model accuracy score\n",
        "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "# report the mean performance\n",
        "print('Accuracy: %.3f' % result.mean())"
      ],
      "metadata": {
        "id": "KUL2PYxAxc-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here is a brief explanation of what each part the code does:\n",
        "\n",
        "    It imports necessary libraries and functions.\n",
        "    It reads the dataset from a CSV file using pandas' read_csv function.\n",
        "    It replaces all 0 values in columns 1 to 5 with NaN (Not a Number), as these might represent missing data.\n",
        "    It removes all rows with any missing values using the dropna function.\n",
        "    It separates the dataset into input features (X) and the target variable (y).\n",
        "    It creates an LDA model.\n",
        "    It sets up a K-Fold cross-validation with 3 splits.\n",
        "    It evaluates the model using cross-validation and calculates the mean accuracy.\n",
        "    Finally, it prints the mean accuracy of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "e38Tpd54x_2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remove Rows With Missing Values\n",
        "\n",
        "We need to address the missing values to be able to rerun LDA.\n",
        "\n",
        "The simplest approach for dealing with missing values is to remove entire predictor(s)\n",
        "and/or sample(s) that contain missing values.\n",
        "\n",
        "We can do this by creating a new Pandas DataFrame with the rows containing missing values\n",
        "removed. Pandas provides the **dropna**() function that can be used to drop either columns or\n",
        "rows with missing data. We can use **dropna**() to remove all rows with missing data,"
      ],
      "metadata": {
        "id": "BzDFrh0pxsUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of removing rows that contain missing values\n",
        "from numpy import nan\n",
        "from pandas import read_csv\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# summarize the shape of the raw data\n",
        "print(dataset.shape)\n",
        "# replace '0' values with 'nan'\n",
        "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
        "# drop rows with missing values. Hint: use .dropna(inplace=True)\n",
        "# Your code here\n",
        "# summarize the shape of the data with missing rows removed\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "4ztdALibx3UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a dataset that we could use to evaluate an algorithm sensitive to missing values\n",
        "like LDA. Let's try again!"
      ],
      "metadata": {
        "id": "WbdbAzZAW-bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on data after rows with missing data are removed\n",
        "from numpy import nan\n",
        "from pandas import read_csv\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# load the dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# replace '0' values with 'nan'\n",
        "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
        "# drop rows with missing values\n",
        "dataset.dropna(inplace=True)\n",
        "# split dataset into inputs and outputs\n",
        "values = dataset.values\n",
        "# In this case, values is a 2D array. The : character means \"all rows\" and 0:8 means \"columns from 0 to 7\". So, X = values[:,0:8] is selecting all rows and the first 8 columns of the data to be assigned to X. This is typically done when you want to separate your features (input for your model) from your target variable.\n",
        "X = values[:,0:8]\n",
        "# What values are then assigned to y? What is y?\n",
        "y = values[:,8]\n",
        "# define the model\n",
        "model = LinearDiscriminantAnalysis()\n",
        "\n",
        "# define the model evaluation procedure. Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
        "# The KFold divides all the samples into 'k' groups of samples, called folds. Here, it's dividing the data into 3 folds.\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "\n",
        "# evaluate the model accuracy score. It is fitting the LDA model on the training portion of the fold, making predictions on the test portion of the fold, and then calculating the accuracy of those predictions\n",
        "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "# report the mean performance. This line is printing the mean accuracy of the model across all folds of the cross-validation.\n",
        "# The %.3f is a placeholder for a floating-point number, with 3 digits after the decimal point.\n",
        "# The % operator is then used to insert the mean accuracy (result.mean()) into this placeholder\n",
        "print('Accuracy: %.3f' % result.mean())"
      ],
      "metadata": {
        "id": "kzKlqSoAyAqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}