{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Model_Building_and_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building and Evaluation**\n",
        "By now, you must have learned how to clean and process data/feature set. In this tutorial, we will learn different types of algorithms and the metrics for evaluating their performance.\n",
        "\n",
        "Adapted from Wafiq Syed 2020 [How to use Scikit-Learn Datasets for Machine Learning](https://towardsdatascience.com/how-to-use-scikit-learn-datasets-for-machine-learning-d6493b38eca3) and Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1)."
      ],
      "metadata": {
        "id": "wb7AKu7Awn9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Scikit-learn](https://scikit-learn.org/stable/) is a great package to perform predictive analysis in Python. To use it the package is called sklearn. You will import specific libraries from the package uing from sklearn import <library>\n",
        "For creating graphs and plots we will use matplotlib.pyplot package.\n"
      ],
      "metadata": {
        "id": "5vMWADdnczzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports necessary libraries\n",
        "from sklearn import datasets, metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sets the backend of matplotlib to the 'inline' backend: With this backend,\n",
        "# the output of plotting commands is displayed inline within frontends like\n",
        "# the Jupyter notebook, directly below the code cell that produced it.\n",
        "# The resulting plots will then also be stored in the notebook document.\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "N2UikCwioR_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Example\n",
        "\n",
        "In this example, we’ll be working with the [“Breast Cancer Wisconsin” dataset](https://colab.research.google.com/drive/18WQntcxOb2dqanbT_EQaJ-q11wtJgdAx?authuser=1#scrollTo=5yqLtP6IhsUC). We will import the data and understand how to read it. We will also build a simple ML model that is able to classify cancer scans either as malignant or benign."
      ],
      "metadata": {
        "id": "5yqLtP6IhsUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import “Breast Cancer Wisconsin” dataset\n",
        "\n",
        "We will obtain data from [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).\n",
        "\n",
        "When using sklearn datasets, each dataset has a corresponding function used to load the dataset. These functions follow the same format: \"load_DATASET()\", where DATASET refers to the name of the dataset.\n",
        " The output is a Bunch object (dictionary)\n",
        "\n",
        "In our previous exercises we used !wget to get the data from some repository or website."
      ],
      "metadata": {
        "id": "A1KzcrZvjiBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "data = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "MjAHF4iUjujb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These load functions (such as *load_breast_cancer()*) don't return data in the tabular format, they return a **Bunch** object, a Scikit-Learn's fancy name for a Dictionary.\n",
        "\n",
        "Let's looking into its keys."
      ],
      "metadata": {
        "id": "cedsEPIDj0YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.keys())"
      ],
      "metadata": {
        "id": "d635nrLolJTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the following keys:\n",
        "\n",
        "* **data** is all the feature data (the attributes of the scan that help us identify if the tumor is malignant or benign, such as radius, area, etc.) in a NumPy array\n",
        "* **target** is the target data (the variable you want to predict, in this case whether the tumor is malignant or benign) in a NumPy array,\n",
        "* **feature_names** are the names of the feature variables, in other words names of the columns in data\n",
        "* **target_names** is the name(s) of the target variable(s), in other words name(s) of the target column(s)\n",
        "* **DESCR**, short for DESCRIPTION, is a description of the dataset\n",
        "filename is the path to the actual file of the data in CSV format.\n",
        "\n",
        "It’s important to note that all of Scikit-Learn datasets are divided into data and target. data represents the features, which are the variables that help the model learn how to predict. target includes the actual labels. In our case, the target data is one column classifies the tumor as either 0 indicating malignant or 1 for benign."
      ],
      "metadata": {
        "id": "pfr8dq38lQeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look the description of the dataset"
      ],
      "metadata": {
        "id": "85zHVXi-l6pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.DESCR)"
      ],
      "metadata": {
        "id": "bxUHIdzhl9iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Working with the Dataset\n",
        "\n",
        "We can use *pandas* to explore the dataset."
      ],
      "metadata": {
        "id": "85QYSAnAmLOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "# Read the DataFrame, first using the feature data\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# Add a target column, and fill it with the target data. 0 encodes for malignant or 1 encodes for benign.\n",
        "df['target'] = data.target\n",
        "# Show the first five rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Lliic-L5mV8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the value of this dataset, run"
      ],
      "metadata": {
        "id": "R5UT46EnmbJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "KYFUBDoAmfDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's do some machine learning (ML)\n",
        "\n",
        "Let’s build a model that classifies cancer tumors as malignant (spreading) or benign (non-spreading). This will show you how to use the data for your own models. We’ll build a simple K-Nearest Neighbors model."
      ],
      "metadata": {
        "id": "YLi_XRxJmxKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let’s split the dataset into two, one for training the model — giving it data to learn from, and the second for testing the model — seeing how well the model performs on data (scans) it hasn’t seen before."
      ],
      "metadata": {
        "id": "RKzRQ2UDm2N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the feature data\n",
        "X = data.data\n",
        "# store the target data\n",
        "y = data.target\n",
        "# split the data using Scikit-Learn's train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "#split is 70%train, 30%test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print('size of train and test datasets', X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "F7N00PFYm4eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us two datasets —one for training and one for testing. Let’s get onto training the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "oAEqd7Aum_G7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM model\n",
        "\n",
        "Lets try classifying with [support vector machine classifier](https://youtu.be/Y6RRHw9uN9o) first.\n",
        "\n",
        "For classification we will use linear SVM (kernel=\"linear\"). We also need to set up a regularization parameter denoted as 'C' in SVM, to control the trade-off between achieving a low training error and a low testing error, that is, the ability to generalize your classifier to unseen data. C takes positive values. A very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. This sometimes help to prevent overfitting to the training data and improve the model's ability to generalize to unseen data So, by adjusting the value of C, you can control the balance between keeping the decision boundary smooth and minimizing the classification error.\n"
      ],
      "metadata": {
        "id": "ejrKx46kn3do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "#rememeber 'random_state' parameter is used for reproducibility of the results each time we run this cell\n",
        "svm = SVC(kernel=\"linear\", C=0.025, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "#score returns the mean accuracy\n",
        "svm.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "uO-krWNEme3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K nearest neighbor classifier\n",
        "Let's try now a different algorithm. The [K nearest neighbor classifier](https://www.ibm.com/topics/knn). To apply this method, we need to import KNeighborsClassifier from sklearn.neighbors. Important parameters is number of neighbors (usually called k). If you set n_neighbors to a small value, like 1 or 2, the model might be too sensitive to noise in the data. On the other hand, if you set n_neighbors to a large value, the model might include points from other classes, which could lead to misclassification.\n",
        "\n",
        "A common practice is to try different values of n_neighbors (like 3, 5, 7, etc.) and see which one gives the best performance on your validation set. This process is known as hyperparameter tuning."
      ],
      "metadata": {
        "id": "CnamejFFr0CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "logreg = KNeighborsClassifier(n_neighbors=6)\n",
        "logreg.fit(X_train, y_train)\n",
        "logreg.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "K5BosYABnBci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUESTION:** Which method gives a better mean accuracy?"
      ],
      "metadata": {
        "id": "G8Q9NARHbCYK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woHDCO6HSi3X"
      },
      "source": [
        "## Classification Model Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check other metrics. We will repeat some steps from previous to keep things complete"
      ],
      "metadata": {
        "id": "cAh0QJIJbO-L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_-9-B-fSi3Y"
      },
      "outputs": [],
      "source": [
        "# let’s first prepare train and test datasets to build our classification models.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "#rememeber 'random_state' parameter is used for reproducibility of the results each time we run this cell\n",
        "svm = SVC(kernel=\"linear\", C=0.025, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "print('size of train and test datasets', X_train.shape, X_test.shape)\n",
        "#score returns the mean accuracy\n",
        "svm.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKa9aONJSi3Y"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display the confusion matrix for the model predictions on the test dataset.\n",
        "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
        "\n",
        "    total_classes = len(classes)\n",
        "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
        "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels,\n",
        "                                  labels=classes)\n",
        "    cm_frame = pd.DataFrame(data=cm,\n",
        "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes],\n",
        "                                                  codes=level_labels),\n",
        "                            index=pd.MultiIndex(levels=[['Actual:'], classes],\n",
        "                                                codes=level_labels))\n",
        "    print(cm_frame)\n",
        "\n",
        "\n",
        "# predict on test data and view confusion matrix\n",
        "y_pred = svm.predict(X_test)\n",
        "display_confusion_matrix(true_labels=y_test, predicted_labels=y_pred, classes=[0, 1])"
      ],
      "metadata": {
        "id": "hC0bPTA0D_fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix should be read as follows:\n",
        "\n",
        "    In 'Actual: 0, Predicted: 0' - The model correctly predicted 60 instances where the actual class was '0'.\n",
        "    In 'Actual: 0, Predicted: 1' - The model incorrectly predicted 3 instances as class '1' when they were actually class '0'.\n",
        "    In 'Actual: 1, Predicted: 0' - The model incorrectly predicted 2 instances as class '0' when they were actually class '1'.\n",
        "    In 'Actual: 1, Predicted: 1' - The model correctly predicted 106 instances where the actual class was '1'.\n",
        "\n",
        "This matrix helps you understand where your model is making mistakes. It's especially useful for imbalanced datasets where accuracy alone can be misleading.\n",
        "Specific example: Out of the 63 observations with label 0 (malignant), our model has correctly predicted 60 observations. Similarly out of\n",
        "Out of the 108 observations with label 1 (benign), our model has correctly predicted 106 observations"
      ],
      "metadata": {
        "id": "YJXAnEB5FN9G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLlOgmXWSi3Y"
      },
      "source": [
        "### True Positive, False Positive, True Negative and False Negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bOgLhEjSi3Z"
      },
      "outputs": [],
      "source": [
        "# considering 1 as our positive class\n",
        "positive_class = 1\n",
        "# True Positive (TP): This is the count of the total number of instances from the\n",
        "#positive class where the true class label was equal to the predicted class label.\n",
        "TP = 106\n",
        "\n",
        "# False Positive (FP): This is the count of the total number of instances from the\n",
        "#negative class where our model misclassified them by predicting them as positive.\n",
        "FP = 3\n",
        "\n",
        "# True Negative (FN): This is the count of the total number of instances from the\n",
        "# negative class where the true class label was equal to the predicted class label.\n",
        "TN = 60\n",
        "\n",
        "# False Negative (FN): This is the count of the total number of instances from the\n",
        "# positive class where our model misclassified them by predicting them as negative.\n",
        "FN = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp2pZEo7Si3Z"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "This is one of the most popular measures of classifier performance. It is defined as the overall\n",
        "accuracy or proportion of correct predictions of the model. The formula for computing accuracy from the\n",
        "confusion matrix is:\n",
        "\n",
        "$Accurcy=\\frac{TP+TN}{TP+FP+TN+FN}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLjfbgD6Si3Z"
      },
      "outputs": [],
      "source": [
        "fw_acc = round(metrics.accuracy_score(y_true=y_test, y_pred=y_pred), 5)\n",
        "mc_acc = round((TP + TN) / (TP + TN + FP + FN), 5)\n",
        "print('Framework Accuracy:', fw_acc)\n",
        "print('Manually Computed Accuracy:', mc_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pjMVMm-Si3Z"
      },
      "source": [
        "### Precision\n",
        "\n",
        "Precision, also known as positive predictive value, is another metric that can be derived from\n",
        "the confusion matrix. It is defined as the number of predictions made that are actually correct or relevant out\n",
        "of all the predictions based on the positive class. The formula for precision is as follows:\n",
        "\n",
        "$Precision=\\frac{TP}{TP+FP}$\n",
        "\n",
        "A model with high precision will identify a higher fraction of positive class as compared to a model\n",
        "with a lower precision. Precision becomes important in cases where we are more concerned about finding\n",
        "the maximum number of positive class even if the total accuracy reduces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfQO98XqSi3a"
      },
      "outputs": [],
      "source": [
        "fw_prec = round(metrics.precision_score(y_true=y_test, y_pred=y_pred), 5)\n",
        "mc_prec = round((TP) / (TP + FP), 5)\n",
        "print('Framework Precision:', fw_prec)\n",
        "print('Manually Computed Precision:', mc_prec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqq3NrtSi3a"
      },
      "source": [
        "### Recall\n",
        "\n",
        "Recall, also known as sensitivity, is a measure of a model to identify the percentage of relevant\n",
        "data points. It is defined as the number of instances of the positive class that were correctly predicted. This is\n",
        "also known as hit rate, coverage, or sensitivity. The formula for recall is:\n",
        "\n",
        "$Recall=\\frac{TP}{TP+FN}$\n",
        "\n",
        "Recall becomes an important measure of classifier performance in scenarios where we want to catch\n",
        "the most number of instances of a particular class even when it increases our false positives. For example,\n",
        "consider the case of bank fraud, a model with high recall will give us higher number of potential fraud cases.\n",
        "But it will also help us raise alarm for most of the suspicious cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW16OZmISi3a"
      },
      "outputs": [],
      "source": [
        "fw_rec = round(metrics.recall_score(y_true=y_test, y_pred=y_pred), 5)\n",
        "mc_rec = round((TP) / (TP + FN), 5)\n",
        "print('Framework Recall:', fw_rec)\n",
        "print('Manually Computed Recall:', mc_rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtVLsRUvSi3a"
      },
      "source": [
        "### F1-Score\n",
        "\n",
        "There are some cases in which we want a balanced optimization of both precision and recall.\n",
        "F1 score is a metric that is the harmonic mean of precision and recall and helps us optimize a classifier for\n",
        "balanced precision and recall performance.\n",
        "The formula for the F1 score is:\n",
        "\n",
        "$F1 Score = \\frac{2 x Precision x Recall}{Precision + Recall}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE7PIzFISi3a"
      },
      "outputs": [],
      "source": [
        "fw_f1 = round(metrics.f1_score(y_true=y_test, y_pred=y_pred), 5)\n",
        "mc_f1 = round((2*mc_prec*mc_rec) / (mc_prec+mc_rec), 5)\n",
        "print('Framework F1-Score:', fw_f1)\n",
        "print('Manually Computed F1-Score:', mc_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T73qPN93Si3U"
      },
      "source": [
        "## Clustering Example\n",
        "\n",
        "In this example, we will learn how we can fit a clustering model on “Breast Cancer Wisconsin” dataset. We will use a labeled dataset to help us see the results of the clustering model and compare it with actual labels. A point to remember here is that, usually labeled data is not available in the real world,\n",
        "which is why we choose to go for unsupervised methods like clustering. We will try to cover two different\n",
        "algorithms, one each from partitioning based clustering and hierarchical clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90lcNCb3Si3U"
      },
      "outputs": [],
      "source": [
        "# Load Wisconsin Breast Cancer Dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# load data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "print(X.shape, data.feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident that we have a total of 569 observations and 30 attributes or features for each observation."
      ],
      "metadata": {
        "id": "8x1Iun2oyXA3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBw9M4CFSi3U"
      },
      "source": [
        "### Partition based Clustering Example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will choose the simplest yet most popular partition based clustering model for our example, which\n",
        "is **K-means** algorithm. This algorithm is a centroid based clustering algorithm, which starts with some\n",
        "assumption about the total clusters in the data and with random centers assigned to each of the clusters.\n",
        "It then reassigns each data point to the center closest to it, using Euclidean distance as the distance metric.\n",
        "After each reassignment, it recalculates the center of that cluster. The whole process is repeated iteratively\n",
        "and stopped when reassignment of data points doesn’t change the cluster centers. Variants include\n",
        "algorithms like **K-medioids**."
      ],
      "metadata": {
        "id": "urRwv02cj9BF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHLkNJ0CSi3V"
      },
      "outputs": [],
      "source": [
        "# determine these two clusters (either 0 or 1) from the data by K-means clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "km = KMeans(n_clusters=2, random_state=2)\n",
        "km.fit(X)\n",
        "\n",
        "labels = km.labels_\n",
        "centers = km.cluster_centers_\n",
        "# labels of the first 10 data points\n",
        "print(labels[:10])\n",
        "# numerical value of the dimensions of the data\n",
        "# (the 30 attributes in the dataset) around which data is clustered.\n",
        "print(centers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH6CryJlSi3V"
      },
      "outputs": [],
      "source": [
        "# we will leverage PCA to reduce the input dimensions (30) to two principal components\n",
        "# and visualize the clusters on top of the same.\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "bc_pca = pca.fit_transform(X)\n",
        "fig = px.scatter(bc_pca, x=0, y=1, color=df['target'])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVR3T8uiSi3V"
      },
      "outputs": [],
      "source": [
        "# visualize the clusters on the reduced 2D feature space for the actual labels as\n",
        "# well as the clustered output labels.\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
        "fig.suptitle('Visualizing breast cancer clusters')\n",
        "fig.subplots_adjust(top=0.85, wspace=0.5)\n",
        "ax1.set_title('Actual Labels')\n",
        "ax2.set_title('Clustered Labels')\n",
        "\n",
        "for i in range(len(y)):\n",
        "    if y[i] == 0:\n",
        "        c1 = ax1.scatter(bc_pca[i,0], bc_pca[i,1],c='g', marker='.')\n",
        "    if y[i] == 1:\n",
        "        c2 = ax1.scatter(bc_pca[i,0], bc_pca[i,1],c='r', marker='.')\n",
        "\n",
        "    if labels[i] == 0:\n",
        "        c3 = ax2.scatter(bc_pca[i,0], bc_pca[i,1],c='g', marker='.')\n",
        "    if labels[i] == 1:\n",
        "        c4 = ax2.scatter(bc_pca[i,0], bc_pca[i,1],c='r', marker='.')\n",
        "\n",
        "l1 = ax1.legend([c1, c2], ['0', '1'])\n",
        "l2 = ax2.legend([c3, c4], ['0', '1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that the clustering has worked quite well and it shows distinct\n",
        "separation between clusters with labels 0 and 1 and is quite similar to the actual labels. However we do\n",
        "have some overlap where we have mislabeled some instances.\n",
        "\n",
        "Remember in an actual real-world scenario, you will not have the actual labels to compare with and the\n",
        "main idea is to find structures or patterns in your data in the form of these clusters.\n",
        "Hence even when dealing with labeled data and running clustering do not\n",
        "compare clustered label values with actual labels and try to measure accuracy.\n",
        "\n",
        "Another very important\n",
        "point to remember is that cluster label values have no significance. The labels 0 and 1 are just values to\n",
        "distinguish cluster data points from each other.\n",
        "\n",
        "Also another important note\n",
        "is that if we had asked for more than two clusters, the algorithm would have readily supplied more clusters\n",
        "but it would have been hard to interpret those and many of them would not make sense. Hence, one of\n",
        "the caveats of using the K-means algorithm is to use it in the case where we have some idea about the total\n",
        "number of clusters that may exist in the data."
      ],
      "metadata": {
        "id": "62jQBDvooW6u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OehgxWqRSi3b"
      },
      "source": [
        "## Clustering Model Evaluation Metrics\n",
        "\n",
        "The lack of a validated ground truth, i.e. the abscence of true labels in the data makes the  evaluation of clustering (or unsupervised models in general) very difficult.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM3qKb9DSi3b"
      },
      "source": [
        "### External validation\n",
        "\n",
        "External validation means validating the clustering model when we have some ground truth available\n",
        "as labeled data. The presence of external labels reduces most of the complexity of model evaluation as\n",
        "the clustering (unsupervised) model can be validated in similar fashion to classification models.\n",
        "\n",
        "Three popular metrics can be used in this scenario:\n",
        "\n",
        "* **Homogeneity**: A clustering model prediction result satisfies homogeneity if all of\n",
        "its clusters contain only data points that are members of a single class (based on the\n",
        "true class labels).\n",
        "* **Completeness**: A clustering model prediction result satisfies completeness if\n",
        "all the data points of a specific ground truth class label are also elements of the\n",
        "same cluster.\n",
        "* **V-measure**: The harmonic mean of homogeneity and completeness scores gives us\n",
        "the V-measure value.\n",
        "\n",
        "Values are typically bounded between 0 and 1 and usually higher values are better. Let’s compute these\n",
        "metric on our two K-means clustering models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate these metrics for the Kmeans classification of the breast cancer data we ran before (variables X for the data and y for the observation labels)."
      ],
      "metadata": {
        "id": "TxcRvd2Flxi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#km_labels = km.labels_ is getting the labels for each point in the dataset. These labels represent which cluster each point belongs to.\n",
        "km_labels = km.labels_\n",
        "#calculating the homogeneity, completeness, and V-measure of the clustering.These are metrics used to evaluate the quality of the clustering.\n",
        "km_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km_labels), 3)\n",
        "print('Homogeneity, Completeness, V-measure metrics for num clusters=2: ', km_hcv)"
      ],
      "metadata": {
        "id": "OjyoLVzDk7GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzTt2KQ-Si3b"
      },
      "source": [
        "### Build two clustering models on the breast cancer dataset\n",
        "\n",
        "We will now compare the two K-means models:\n",
        "one with two clusters and the second one with five clusters—and\n",
        "then evaluate their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PouCtWvKSi3b"
      },
      "outputs": [],
      "source": [
        "km2 = KMeans(n_clusters=2, random_state=42).fit(X)\n",
        "km2_labels = km2.labels_\n",
        "\n",
        "km5 = KMeans(n_clusters=5, random_state=42).fit(X)\n",
        "km5_labels = km5.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw66wxnbSi3c"
      },
      "outputs": [],
      "source": [
        "km2_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km2_labels), 3)\n",
        "km5_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km5_labels), 3)\n",
        "\n",
        "print('Homogeneity, Completeness, V-measure metrics for num clusters=2: ', km2_hcv)\n",
        "print('Homogeneity, Completeness, V-measure metrics for num clusters=5: ', km5_hcv)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the V-measure for the first model with two clusters is better than the one with five\n",
        "clusters and the reason is because of higher completeness score."
      ],
      "metadata": {
        "id": "vWWvn9fBP6Zz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YPqmfrYSi3c"
      },
      "source": [
        "### Internal validation\n",
        "\n",
        "Internal validation means validating a clustering model by defining metrics that capture the expected\n",
        "behavior of a good clustering model. A good clustering model can be identified by two very desirable traits:\n",
        "* Compact groups, i.e. the data points in one cluster occur close to each other.\n",
        "* Well separated groups, i.e. two groups\\clusters have as large distance among\n",
        "them as possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####Silhouette Coefficient\n",
        "Silhouette coefficient is a metric that tries to combine the two requirements of a good clustering model. The\n",
        "silhouette coefficient is defined for each sample and is a combination of its similarity to the data points in its\n",
        "own cluster and its dissimilarity to the data points not in its cluster.\n",
        "\n",
        "The silhouette coefficient is usually bounded between -1 (incorrect clustering) and +1 (excellent quality\n",
        "dense clusters). A higher value of silhouette coefficient generally means that the clustering model is leading\n",
        "to clusters that are dense and well separated and distinguishable from each other. Lower scores indicate\n",
        "overlapping clusters."
      ],
      "metadata": {
        "id": "oeuthB1TQKYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emEeyloaSi3c"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics\n",
        "\n",
        "km2_silc = metrics.silhouette_score(X, km2_labels, metric='euclidean')\n",
        "km5_silc = metrics.silhouette_score(X, km5_labels, metric='euclidean')\n",
        "\n",
        "print('Silhouette Coefficient for num clusters=2: ', km2_silc)\n",
        "print('Silhouette Coefficient for num clusters=5: ', km5_silc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that from the metric results it seems like we have better\n",
        "cluster quality with two clusters as compared to five clusters."
      ],
      "metadata": {
        "id": "IxONGqSvQjGJ"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}