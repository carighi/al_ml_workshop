{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carighi/al_ml_workshop/blob/main/Feature_Scaling_Data_with_Outliers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scale Data with Outliers**\n",
        "\n",
        "Many machine learning algorithms perform better when numerical input variables are scaled to\n",
        "a standard range. This includes algorithms that use a weighted sum of the input, like linear\n",
        "regression, and algorithms that use distance measures, like k-nearest neighbors. Standardizing\n",
        "is a popular scaling technique that subtracts the mean from values and divides by the standard\n",
        "deviation, transforming the probability distribution for an input variable to a standard Gaussian\n",
        "(zero mean and unit variance). Standardization can become skewed or biased if the input\n",
        "variable contains outlier values.\n",
        "To overcome this, the median and interquartile range can be used when standardizing\n",
        "numerical input variables, generally referred to as robust scaling.\n",
        "\n",
        "In this tutorial, you will learn:\n",
        "\n",
        "* Many machine learning algorithms prefer or perform better when numerical input variables\n",
        "are scaled.\n",
        "* Robust scaling techniques that use percentiles to scale numerical input\n",
        "variables that contain outliers.\n",
        "* How to use the RobustScaler to scale numerical input variables using the median and\n",
        "interquartile range.\n",
        "\n",
        "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
        "\n"
      ],
      "metadata": {
        "id": "mfyjWBlmvoQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Robust Scaling Data\n",
        "If there are input\n",
        "variables that have very large values relative to the other input variables, these large values\n",
        "can dominate or skew some machine learning algorithms. The result is that the algorithms pay\n",
        "most of their attention to the large values and ignore the variables with smaller values.\n",
        "\n",
        "Sometimes an input variable may have outlier values. Outliers can skew a probability distribution and make data scaling using standardization\n",
        "difficult as the calculated mean and standard deviation will be skewed by the presence of\n",
        "the outliers.\n",
        "\n",
        "Robust standardization or robust data\n",
        "scaling that calculates the median (50th percentile) and the 25th and\n",
        "75th percentiles. The values of each variable then have their median subtracted and are divided\n",
        "by the interquartile range (IQR) which is the difference between the 75th and 25th percentiles\n"
      ],
      "metadata": {
        "id": "YrJzhZyMLjqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Diabetes Dataset\n",
        "The dataset classifies patient data as\n",
        "either an onset of diabetes within five years or not. There are 768 examples and eight input variables. It is a binary classification problem.\n",
        "\n",
        "You can learn more about the dataset here:\n",
        "\n",
        "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
        "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
      ],
      "metadata": {
        "id": "mGiANfntOp8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Diabetes data files"
      ],
      "metadata": {
        "id": "Uy-6nuyTOufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -O pima-indians-diabetes.csv\n",
        "!wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -O pima-indians-diabetes.names\n",
        "!head pima-indians-diabetes.csv"
      ],
      "metadata": {
        "id": "SCY6UaZVOxjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and summarize the diabetes dataset\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# summarize the shape of the dataset\n",
        "print(dataset.shape)\n",
        "# summarize each variable\n",
        "print(dataset.describe())"
      ],
      "metadata": {
        "id": "ApYh_jX8PAeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms the 8\n",
        "input variables, one output variable, and 768 rows of data. A statistical summary of the input\n",
        "variables is provided show that each variable has a very different scale. This makes it a good\n",
        "dataset for exploring data scaling methods."
      ],
      "metadata": {
        "id": "Bi4e3d_0PKst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a histogram for each input variable. The plots confirm the differing scale\n",
        "for each input variable and show that the variables have differing scales. Importantly, we can see\n",
        "some of the distributions show the presence of outliers. The dataset provides a good candidate for using a robust scaler transform to standardize the data in the presence of di ering input\n",
        "variable scales and outliers."
      ],
      "metadata": {
        "id": "_p5skT8jPTEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "1sdeq7YZPPFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
        "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
        "stratified k-fold cross-validation."
      ],
      "metadata": {
        "id": "P2gO--wiPeyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate knn on the raw diabetes dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define and configure the model\n",
        "model = KNeighborsClassifier()\n",
        "# evaluate the model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report model performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "eNIn8GxrPi8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we can see that the model achieved a mean classi cationfiaccuracy of about\n",
        "71.7 percent.\n",
        "\n",
        "Next, let's explore a robust scaling transform of the dataset."
      ],
      "metadata": {
        "id": "cCfD9x1wPpn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IQR Robust Scaler Transform\n",
        "We can apply the robust scaler to the diabetes dataset directly. We will use the default\n",
        "configuration and scale values to the IQR. First, a **RobustScaler** instance is defined with\n",
        "default hyperparameters. Once defined, we can call the **fit.transform()** function and pass it\n",
        "to our dataset to create a robust scale transformed version of our dataset."
      ],
      "metadata": {
        "id": "-79_v3GVPzPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize a robust scaler transform of the diabetes dataset\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# retrieve just the numeric input values\n",
        "data = dataset.values[:, :-1]\n",
        "# perform a robust scaler transform of the dataset\n",
        "trans = RobustScaler()\n",
        "data = trans.fit_transform(data)\n",
        "# convert the array back to a dataframe\n",
        "dataset = DataFrame(data)\n",
        "# summarize\n",
        "print(dataset.describe())"
      ],
      "metadata": {
        "id": "-mU8XSxeQBoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the\n",
        "distributions have been adjusted. The median values are now zero and the standard deviation\n",
        "values are now close to 1.0."
      ],
      "metadata": {
        "id": "dn5O0nC3QIrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histograms of the variables\n",
        "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
        "[x.title.set_size(4) for x in fig.ravel()]\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "Xo2zE68zQRKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram plots of the variables are created, although the distributions don't look much\n",
        "different from their original distributions seen in the previous section. We can see that the\n",
        "center of mass for each distribution is now close to zero."
      ],
      "metadata": {
        "id": "YIdlSqlHQOFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
        "robust scaler transform of the dataset."
      ],
      "metadata": {
        "id": "Xg-QDROsQW5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate knn on the diabetes dataset with robust scaler transform\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "data = dataset.values\n",
        "# separate into input and output columns\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# ensure inputs are floats and output is an integer label\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "# define the pipeline\n",
        "trans = RobustScaler()\n",
        "model = KNeighborsClassifier()\n",
        "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
        "# evaluate the pipeline\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report pipeline performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "dEFGhmgHQXkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the robust scaler transform results in a lift in\n",
        "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
        "transform."
      ],
      "metadata": {
        "id": "-18EPrpaQkNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explore Robust Scaler Range\n",
        "The range used to scale each variable is chosen by default as the IQR is bounded by the 25th\n",
        "and 75th percentiles. This is specified by the quantile range argument as a tuple. Other\n",
        "values can be specified and might improve the performance of the model, such as a wider range,\n",
        "allowing fewer values to be considered outliers, or a more narrow range, allowing more values\n",
        "to be considered outliers. The example below explores the effect of different definitions of the\n",
        "range from 1st to the 99th percentiles to 30th to 70th percentiles."
      ],
      "metadata": {
        "id": "y43yS8WnQrdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# explore the scaling range of the robust scaler transform\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "  # load dataset\n",
        "  dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "  data = dataset.values\n",
        "  # separate into input and output columns\n",
        "  X, y = data[:, :-1], data[:, -1]\n",
        "  # ensure inputs are floats and output is an integer label\n",
        "  X = X.astype('float32')\n",
        "  y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "  return X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "  models = dict()\n",
        "  for value in [1, 5, 10, 15, 20, 25, 30]:\n",
        "  # define the pipeline\n",
        "    trans = RobustScaler(quantile_range=(value, 100-value))\n",
        "    model = KNeighborsClassifier()\n",
        "    models[str(value)] = Pipeline(steps=[('t', trans), ('m', model)])\n",
        "  return models\n",
        "\n",
        "# evaluate a given model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "  return scores\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "  scores = evaluate_model(model, X, y)\n",
        "  results.append(scores)\n",
        "  names.append(name)\n",
        "  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n"
      ],
      "metadata": {
        "id": "E92hJkgmQu8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that ranges such as 10-90 and 15-85 perform better than the default\n",
        "of 25-75."
      ],
      "metadata": {
        "id": "7uoutRSZRIvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "3HUR4AjvRLaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box and whisker plots are created to summarize the classification accuracy scores for each\n",
        "IQR range. We can see a subtle difference in the distribution and mean accuracy with the larger\n",
        "ranges of 15-85 vs 25-75. percentiles."
      ],
      "metadata": {
        "id": "o9U4MNhHRbMO"
      }
    }
  ]
}